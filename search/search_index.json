{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Software engineers often encounter problems which require the dissemination of small or moderately sized data sets which don\u2019t fit the label \u201cbig data\u201d.  To solve these problems, we often send the data to an RDBMS or nosql data store and query it at runtime, or serialize the data as json or xml, distribute it, and keep a local copy on each consumer.</p> <p>Scaling each of these solutions presents different challenges.  Sending the data to an RDBMS, nosql data store, or even a memcached cluster may allow your dataset to grow indefinitely large, but there are limitations on the latency and frequency with which you can interact with that dataset.  Serializing and keeping a local copy (if in RAM) can allow many orders of magnitude lower latency and higher frequency access, but this approach has many scaling challenges:</p> <ul> <li>The dataset size is limited by available RAM.</li> <li>The full dataset may need to be re-downloaded each time it is updated.</li> <li>Updating the dataset may require significant CPU resources or impact GC behavior.</li> </ul> <p>Netflix, serving many billions of personalized requests each day, has a few use cases for which the latency of a remote datastore would be highly undesirable given the frequency with which those datasets are accessed.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>Netflix Hollow is a java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Hollow aggressively addresses the scaling challenges of in-memory datasets, and is built with servers busily serving requests at or near maximum capacity in mind.</p> <p>Due to its performance characteristics, Hollow shifts the scale in terms of appropriate dataset sizes for an in-memory solution.  Datasets for which such liberation may never previously have been considered can be candidates for Hollow.  </p> <p>Small to Medium Datasets</p> <p>Hollow may be entirely appropriate for datasets which, if represented with json or XML, might require in excess of 100GB.  A good rule of thumb in 2017: KB, MB, and often GB, but not TB or PB.</p> <p>Hollow simultaneously targets three goals:</p> <ul> <li>Maximum development agility</li> <li>Highly optimized performance and resource management</li> <li>Extreme stability and reliability</li> </ul>"},{"location":"#maximum-agility","title":"Maximum Agility","text":"<p>Hollow provides the capability to automatically generate a custom API based on a specific data model, so that consumers can intuitively interact with the data, with the benefit of IDE code completion.</p> <p>Hollow provides insight tools, which will help users understand their dataset, and how it changes over time, more deeply than ever before:</p> <ul> <li>Comprehensive change history</li> <li>Diffing entire datasets between arbitrary states</li> <li>Heap usage analysis</li> <li>Usage tracking</li> </ul> <p>The toolset available for working with Hollow datasets allows for a surprising variety of operations to be performed with ease, including:</p> <ul> <li>Indexing / Querying for individual records in a dataset</li> <li>Splitting / Combining entire datasets in many different ways</li> <li>Filtering individual record types at the consumer to reduce heap footprint</li> </ul>"},{"location":"#optimized-performance","title":"Optimized Performance","text":"<p>Hollow is hyper-optimized with a few performance metrics at top-of-mind:</p> <ul> <li>Heap footprint</li> <li>Computational cost of access</li> <li>GC impact of updates</li> <li>Computational cost of updates</li> <li>Network cost of updates</li> </ul> <p>Over time, Hollow automatically calculates the changes in a dataset on the producer.  Instead of retransmitting the entire snapshot of the data for each update, only the changes are disseminated to consumers to keep them up to date.</p> <p>On consumers, Hollow keeps a compact encoding of the dataset in RAM.  This representation is optimized for both minimizing heap footprint and minimizing access CPU cost.  To retain and keep the dataset updated, Hollow pools and reuses heap memory to avoid GC tenuring.</p>"},{"location":"#extreme-stability","title":"Extreme Stability","text":"<p>Hollow has been battle-hardened over more than two years of continuous use at Netflix.  Hollow is used to represent crucial datasets, essential to the fulfillment of the Netflix experience, on servers answering live customer requests.  So although Hollow goes to extraordinary lengths to squeeze every last bit of performance out of servers\u2019 hardware, enormous attention to detail has gone into solidifying this foundational piece of our infrastructure.</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>Hollow is originally created by Drew Koszewnik with the advice and support of fellow members of the Data Platform Technologies team at Netflix:</p> <ul> <li>David Su</li> <li>Deva Jayaraman</li> <li>Jatin Shah</li> <li>Kinesh Satiya</li> <li>Lavanya Kanchanapalli</li> <li>Ramin Forood</li> <li>Rohit Kaul</li> <li>Tim Taylor</li> </ul> <p>Hollow is maintained by the Data Platform Technologies team at Netflix.</p> <p>Hollow makes use of an implementation of the MurmurHash3 algorithm authored by Yonik Seeley.</p> <p>Hollow makes use of Thomas Wang's well known 32-bit mix function, which is based on an original suggestion by Robert Jenkins.</p> <p>The hollow-diff-ui object diff view was inspired in part by Chas Emerick's jsdifflib, and borrows some of the .css from that project.</p> <p>This documentation was created with MkDocs, using a theme from material.</p>"},{"location":"advanced-topics/","title":"Advanced Topics","text":"<p>This section covers few topics in details to give a deep insight into implementation of Hollow.</p>"},{"location":"advanced-topics/#schema-parser","title":"Schema Parser","text":"<p>Hollow schemas can be serialized as Java Strings.  Calling <code>toString()</code> on a <code>HollowSchema</code> will produce a human-readable representation of the schema.  The following shows the String representations of all of the schemas from a <code>Movie</code>/<code>Actor</code> example data model:</p> <pre><code>Movie @PrimaryKey(id) {\n    long id;\n    int releaseYear;\n    string title;\n    SetOfActor actors;\n}\n\nSetOfActor Set&lt;Actor&gt; @HashKey(firstname, surname);\n\nActor {\n    long id;\n    String firstname;\n    String surname;\n}\n\nString {\n    string value;\n}\n</code></pre> <p>These representations can be parsed using the <code>HollowSchemaParser</code>, and in turn can be used to initialize the state of a <code>HollowWriteStateEngine</code>: <pre><code>String allSchemas = /// a String containing all schemas\n\nList&lt;HollowSchema&gt; schemas =\n             HollowSchemaParser.parseCollectionOfSchemas(allSchemas);\n\nHollowWriteStateEngine initializedWriteEngine =\n             HollowWriteStateCreator.createWithSchemas(schemas);\n</code></pre></p> <p>Guiding Data Ingestion with a Data Model</p> <p>For a generic data ingestion mechanism, loading the schemas from a text representation comes in handy.  For example, the JSON to Hollow adapter requires a <code>HollowWriteStateEngine</code> which is preinitialized with a data model.  The data model can be configured in a text file, and loaded with the <code>HollowSchemaParser</code>.</p> <p>Object schema definitions take the following form: <pre><code>ObjectTypeName @PrimaryKey(fieldName1, fieldNameN) {\n   FieldType1 fieldName1;\n   FieldType2 fieldName2;\n   ...\n   FieldTypeN fieldNameN;\n}\n</code></pre></p> <p>The primary key definition is optional and should be omitted if no primary key should be defined for a type.</p> <p>Object schemas may define any of the following field types: <code>int</code>, <code>long</code>, <code>float</code>, <code>double</code>, <code>boolean</code>, <code>string</code>, <code>bytes</code>.  If a field has a type other than these, the field will be interpreted as a <code>REFERENCE</code> to another type of that name.</p> <p>Lowercase Field Type Declarations</p> <p>Note that the declarations for each of the inline field types are all lowercase (including <code>string</code>).  An uppercase letter in any of these types will be interpreted as a <code>REFERENCE</code> field to a separate type.</p> <p><code>List</code>, <code>Set</code>, and <code>Map</code> types use the following notation:</p> <ul> <li><code>ListTypeName List&lt;ElementTypeName&gt;;</code></li> <li><code>SetTypeName Set&lt;ElementTypeName&gt; @HashKey(elementFieldOne, elementFieldTwo);</code></li> <li><code>MapTypeName Map&lt;KeyTypeName, ValueTypeName&gt; @HashKey(keyFieldOne, keyFieldTwo);</code></li> </ul> <p><code>Set</code> and <code>Map</code> types may optionally define a hash key.  The hash key definition should be omitted if no hash key should be defined for a type.</p> <p>Elements, keys, and values in collection record types cannot be inlined.  Whitespace is unimportant when parsing schema definitions.</p>"},{"location":"advanced-topics/#low-level-input-api","title":"Low Level Input API","text":"<p>Although Hollow includes a few ready-made data ingestion utilities, other data ingestion utilities can be created.  Adding data into Hollow starts with a <code>HollowWriteStateEngine</code>.  We need to initialize a type state for each schema in our data model: <pre><code>HollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\n\nHollowObjectSchema movieSchema = new HollowObjectSchema(\"Movie\", 3);\nmovieSchema.addField(\"id\", FieldType.LONG);\nmovieSchema.addField(\"title\", FieldType.REFERENCE, \"String\");\nmovieSchema.addField(\"releaseYear\", FieldType.INT);\n\nHollowObjectTypeWriteState movieState = new HollowObjectTypeWriteState(movieSchema);\n\nwriteEngine.addTypeState(movieState);\n</code></pre></p> <p>Once we\u2019ve initialized our type states, we can add data into our state engine using HollowWriteRecords: <pre><code>HollowObjectSchema stringSchema = /// the String schema\nHollowObjectSchema movieSchema = /// the Movie schema\n\nHollowObjectWriteRecord titleRec = new HollowObjectWriteRecord(stringSchema);\nHollowObjectWriteRecord movieRec = new HollowObjectWriteRecord(movieSchema);\n\ntitleRec.setString(\"value\", \"The Matrix\");\n\nint titleOrdinal = writeEngine.add(\"String\", titleRec);\n\nmovieRec.setLong(\"id\", 1);\nmovieRec.setReference(\"title\", titleOrdinal);\nmovieRec.setInt(\"releaseYear\", 1999);\n\nwriteEngine.add(\"Movie\", movieRec);\n</code></pre></p> <p>Note that referenced records must be added prior to referencing records in order to obtain the referenced ordinals.</p> <p>Reusing HollowWriteRecords</p> <p><code>HollowWriteRecord</code>s can be reused -- just be sure to call <code>reset()</code> before populating data from the next record.</p> <p>Each schema type has its own <code>HollowTypeWriteState</code> and <code>HollowWriteRecord</code> implementation: <pre><code>HollowObjectSchema objectSchema = /// an Object schema\nHollowListSchema listSchema = /// a List schema\nHollowSetSchema setSchema = /// a Set schema\nHollowMapSchema mapSchema = /// a Map schema\n\nHollowObjectTypeWriteState objectTypeState = new HollowObjectTypeWriteState(objectSchema);\nHollowListTypeWriteState listTypeState = new HollowListTypeWriteState(listSchema);\nHollowSetTypeWriteState setTypeState = new HollowSetTypeWriteState(setSchema);\nHollowMapTypeWritestate mapTypeState = new HollowMapTypeWriteState(mapSchema);\n\nHollowObjectWriteRecord objectRec = new HollowObjectWriteRecord(objectSchema);\nHollowListWriteRecord listRec = new HollowListWriteRecord();\nHollowSetWriteRecord setRec = new HollowSetWriteRecord();\nHollowMapWriteRecord mapRec = new HollowMapWriteRecord();\n</code></pre></p> <p>Using this API, it is possible to create a generic data ingestion mechanism from any type of input source.</p>"},{"location":"advanced-topics/#determining-populated-ordinals","title":"Determining Populated Ordinals","text":"<p>Many lower-level operations require knowledge of the currently populated ordinals (i.e. those which are not holes), or knowledge of ordinals which were populated in state prior to the last delta application.  We can determine this from any type state in a <code>HollowReadStateEngine</code>.  Both of these are useful, for example, if consumers wish to inspect exactly what has changed.</p> <p>Each <code>HollowTypeReadState</code> underneath a <code>HollowReadStateEngine</code> contains the methods <code>getPopulatedOrdinals()</code> and <code>getPreviousOrdinals()</code>.  Each of these methods returns a <code>java.util.BitSet</code>.  The contents of returned <code>BitSet</code>s may be inspected, but should never be modified.</p>"},{"location":"advanced-topics/#caching","title":"Caching","text":"<p>Although a lot of effort has gone into minimizing the cost to read Hollow data, there is still inevitably a performance difference between accessing a field in a POJO and accessing a field in Hollow [see also: Performance API]. In general, this will not be a performance bottleneck, but in some rare cases the performance of tight inner loops may suffer on consumers.</p> <p>When there is a type with a low cardinality, we can instantiate and cache a POJO implementation for each ordinal, which can be used by consumers in tight inner loops.  This is accomplished by simply passing a <code>Set&lt;String&gt;</code> as the second constructor argument when instantiating a custom-generated Hollow API.  The elements in the Set should be the types to cache.</p> <p>Avoid Premature Optimization</p> <p>Caching should be used judiciously.  In all but the tightest of loops, caching will be unnecessary, and can even be detrimental to performance for types with a large cardinality.</p>"},{"location":"advanced-topics/#performance-api","title":"Performance API","text":"<p>A specially generated consumer API allows for traversal of a dataset without ever creating POJOs. Instead, record handles are longs (called Refs). The high bits of a Ref contain a type identifier, and the low bits contain the record's ordinal.</p> <p>Performance API can be generated using the <code>HollowPerformanceAPIGenerator</code> class like- <pre><code>HollowDataset dataset = SimpleHollowDataset.fromClassDefinitions(TestRecord.class);  // alternatively, a snapshot from a previous producer run can be used if available\nHollowPerformanceAPIGenerator.newBuilder()\n        .withDataset(dataset)\n        .withAPIClassname(\"TestPerfAPI\")\n        .withPackageName(\"some.package.name\")\n        .withDestination(\"/path/to/generated/sources\")\n        .build()\n        .generateSourceFiles();\n</code></pre></p> <p>For an example of Performance API usage consider this data model- <pre><code>@HollowPrimaryKey(fields=\"id\")\npublic class TestRecord {\n\n    public final int id;\n    @HollowInline\n    public final String strVal;\n    @HollowHashKey(fields= {\"subVal\", \"iSubVal\"})\n    public Set&lt;TestSubRecord&gt; sub;\n\n    public TestRecord(int id, String strVal, int... subVal) {\n        this.id = id;\n        this.strVal = strVal;\n        this.sub = new HashSet&lt;&gt;();\n\n        for(int val : subVal) {\n            sub.add(new TestSubRecord(val, String.valueOf(val)));\n        }\n    }\n\n    @HollowTypeName(name=\"TestSubRecord\")\n    public static class TestSubRecord {\n        @HollowInline final String subVal;\n        final int iSubVal;\n\n        public TestSubRecord(int iSubVal, String subVal) {\n            this.iSubVal = iSubVal;\n            this.subVal = subVal;\n        }\n    }\n}\n</code></pre></p> <p>Performance API usage- <pre><code>HollowConsumer consumer = HollowConsumer.withBlobRetriever(blobRetriever)\n    .withAnnouncementWatcher(announcementWatcher)\n    .withGeneratedAPIClass(TestPerfAPI.class) // in place of the traditional client API\n    .build();\n\nTestPerfAPI api = (TestPerfAPI) consumer.getAPI();\n\nlong ref = api.TestRecord.refForOrdinal(1);\nSystem.out.println(api.TestRecord.getId(ref));\nSystem.out.println(api.TestRecord.getStrVal(ref));\n\nlong subSetRef = api.TestRecord.getSubRef(ref);\n\nHollowPerfReferenceIterator iter = api.SetOfTestSubRecord.iterator(subSetRef);\nwhile(iter.hasNext()) {\n    long subRef = iter.next();\n    System.out.println(api.TestSubRecord.getSubVal(subRef));\n}\n\nSet&lt;Integer&gt; subSet = api.SetOfTestSubRecord.backedSet(subSetRef, api.TestSubRecord::getISubVal, k -&gt; new Object[] { String.valueOf(k), k });\nSystem.out.println(subSet);\nSystem.out.println(subSet.contains(2002));\n</code></pre></p>"},{"location":"advanced-topics/#hollow-heap-effects","title":"Hollow Heap Effects","text":""},{"location":"advanced-topics/#double-snapshots","title":"Double Snapshots","text":"<p>At times, a new state may be produced to which there is no available delta from the previous state.  When this broken delta chain scenario occurs, consumers will by default attempt to load a snapshot to the latest state.  If a consumer loads a snapshot when it currently has a state loaded for the same dataset, we call this a double snapshot.  Double snapshots result in a doubling of the heap usage of your dataset.  This is because Hollow assumes that consumers may be actively using the current state, and it therefore must retain the current state to provide data while the next state is loaded.  Only after the next state is fully loaded can the old state be dropped.</p> <p>If using the <code>Hollow Consumer</code>, you can configure consumers to never attempt a double snapshot.  This is accomplished with a custom <code>DoubleSnapshotConfig</code>.  In this case, you should check for 'stuck' clients in a <code>RefreshListener</code>, so that you may take the appropriate operational action.</p> <p>If a dataset is large, double snapshots should be avoided for performance reasons.  Double snapshots can be entirely avoided by restoring a <code>HollowWriteStateEngine</code> at producer initialization time, and, if necessary, using the <code>HollowStateDeltaPatcher</code> to operationally fill lost or missing deltas in the event an unforeseen issue occurs.</p>"},{"location":"advanced-topics/#type-sharding","title":"Type Sharding","text":"<p>During a delta transition each individual record type builds the entire next state of the type in memory while the current state is retained to answer requests.  This means that the memory pool Hollow reserves needs to be large enough to compose an extra copy of the largest type.  This is a miniaturized version of the double snapshot problem.</p> <p>To address this problem, large types can be transparently sharded into smaller individual chunks, each of which updates independently.  The effect of this feature is that Hollow only needs to retain a large enough memory pool to update the largest chunk across all types.</p> <p>Type sharding can be specified in two ways:  </p> <ul> <li>The recommended way is to specify a target max type shard size via a call to <code>setTargetMaxTypeShardSize(long bytes)</code> on a <code>HollowWriteStateEngine</code> prior to writing the first snapshot.  With this call, the number of shards will be automatically calculated based on the target excess memory pool size.</li> <li>Additionally, the number of shards for a type can be explicitly specified by annotating a POJO with the <code>@HollowShardLargeType</code> annotation when using the <code>HollowObjectMapper</code> for data ingestion.  This can be useful if rapid growth is anticipated in a type.</li> </ul> <p>Within a continuous delta chain, the type sharding configuration cannot be changed.  When a producer restores the previously produced state at startup, then the restored <code>HollowWriteStateEngine</code> will always retain the sharding configuration of the prior state rather than recalculating based on the current size of each type.  Consequently, if the changes in a dataset over time results in a type sharding configuration which is highly suboptimal, it is recommended to start a new delta chain, which may require a double snapshot on all consumers, a simultaneous restart of all consumers, or a new blob namespace to which consumers can migrate over a period of time.</p> <p>Backwards Compatibility</p> <p>Type sharding is new in v2.1.0.  Consumers can read blobs produced by producers v2.1.0 and later as long as type sharding is disabled. If you are sure that all consumers are using v2.1.0 or later, it is safe to turn on type sharding.</p> <p>In order to avoid causing issues for early adopters, the default target max type shard size is currently set to <code>Long.MAX_VALUE</code>.  At a later time, this default will be changed to 25MB.</p>"},{"location":"advanced-topics/#object-longevity","title":"Object Longevity","text":"<p>A Hollow object returned from a generated API contains a reference to the Hollow data store, and an ordinal.  For this reason, if a reference to a Hollow object is retained by the consumer for an extended period of time, and the underlying record changes unexpected results may begin to be returned from these references.  We call Hollow objects which were obtained from a no longer current state stale references.</p> <p>It is best practice to never cache Hollow objects.  However, if it somehow cannot be guaranteed that Hollow Objects will never be cached at the consumer, and guaranteed protection against accidentally cached objects is necessary, then object longevity can be enabled.</p> <p>With object longevity, Hollow objects will, after an update, be backed by a reserved copy of the data at the time the reference was created.  This guarantees that even if a reference is held for a long time, it will continue to return the same data when interrogated.</p> <p>When object longevity is defined, Two durations are defined:</p> <ul> <li>a grace period, and</li> <li>a usage detection period.  </li> </ul> <p>The grace period is defined by its duration, in milliseconds, after a reference becomes stale.  During the grace period, usage of stale references is acceptable.  The usage detection period is defined by its duration, in milliseconds, after the grace period has expired.  During the usage detection period, usage of stale references is unexpected, but will not result in failed interrogations of Hollow objects.  After the usage detection period expires, data will be dropped if no usage was detected in the usage detection period.  If stale references are interrogated after their backing data is dropped, then Exceptions will be thrown.</p> <p>The <code>ObjectLongevityConfig</code>, injected into the <code>HollowConsumer</code> constructor, contains a few methods which are used to configure object longevity behavior:</p> <ul> <li><code>boolean enableLongLivedObjectSupport()</code>: Whether or not object longevity is enabled.</li> <li><code>long gracePeriodMillis()</code>: If object longevity is enabled, this returns the number of milliseconds before the usage of stale objects gets flagged.</li> <li><code>long usageDetectionPeriodMillis()</code>: If long-lived object support is enabled, this defines the number of milliseconds, after the grace period, during which data is still available in stale references, but usage will be flagged.</li> <li><code>boolean dropDataAutomatically()</code>: Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, as long as no usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached, but unused.</li> <li><code>boolean forceDropData()</code>: Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, even if usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached and used.</li> </ul> <p>Your implementation of the <code>ObjectLongevityConfig</code> may be backed by a dynamic configuration and safely change on live running consumers.  For example, <code>forceDropData()</code> may be operationally useful for boxes that are exhibiting memory leaks due to non-critical cached Hollow objects.</p> <p>How It Works</p> <p>When enabled, object longevity is achieved using the <code>HollowHistory</code> data structure, which results in a minimal heap overhead.</p>"},{"location":"advanced-topics/#memory-pooling","title":"Memory Pooling","text":"<p>Under normal operation, Hollow pools and reuses memory to minimize GC effects while updating data.  This pool of memory is kept as arrays on the heap. Each array in the pool has a fixed length.  When a long array or a byte array is required in Hollow, it will stitch together pooled array segments as a <code>SegmentedByteArray</code> or <code>SegmentedLongArray</code>.  These classes encapsulate the details of treating segmented arrays as contiguous ranges of values.</p> <p>In shared-memory mode (only applicable to data consumers), Hollow uses a <code>BlobByteBuffer</code> abstraction which is basically an array of Java's <code>MappedByteBuffer</code>s stitched together to serve as one contiguous <code>ByteBuffer</code>. During initial load a snapshot file is mmap-ed to virtual memory as a single <code>BlobByteBuffer</code>, and other <code>BlobByteBuffer</code>s are created as views into the underlying <code>BlobByteBuffer</code> to access parts of the Hollow dataset. Beyond an initial snapshot load, delta applications are currently not supported.</p>"},{"location":"advanced-topics/#delta-based-producer-input","title":"Delta-Based Producer Input","text":"<p>The Getting Started section of this documentation describes a producer which every so often reads the entire dataset from some source of truth, re-adds all records to a <code>HollowWriteStateEngine</code>, then produces a delta based on the automatically discovered differences in the dataset since the prior cycle.  It is possible, however, that a producer may receive an incoming stream of events which directly indicate the changes to a dataset, obviating the need to scan through the entire source of truth and re-add the entire dataset on each cycle.</p> <p>If desired, a <code>HollowWriteStateEngine</code>\u2019s state can be explicitly modified, rather than recreated, each cycle.  We start such a cycle by re-adding all of the records from the previous cycle to the state engine:</p> <pre><code>HollowWriteStateEngine writeEngine = /// the state engine\n\nwriteEngine.prepareForNextCycle();\nwriteEngine.addAllObjectsFromPreviousCycle();\n</code></pre> <p>We\u2019ll also need an indexed <code>HollowReadStateEngine</code>, which is updated in lock-step with the <code>HollowWriteStateEngine</code>.  The index can be used to retrieve the ordinals of records to be replaced.  These ordinals can be removed from the <code>HollowWriteStateEngine</code>:</p> <pre><code>HollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nHollowPrimaryKeyIndex idx =\n                       new HollowPrimaryKeyIndex(readEngine, \"Movie\", \"id\");\n\nHollowTypeWriteState movieTypeState = writeEngine.getTypeState(\"Movie\");\n\nList&lt;MovieUpdateEvent&gt; eventBatch = /// a batch of events\n\nfor(MovieUpdateEvent event : eventBatch) {\n    int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n    movieTypeState.removeOrdinalFromThisCycle(oldOrdinal);\n    mapper.add(event.getMovie());\n}\n</code></pre> <p>Watch out for Duplicates</p> <p>Be careful, this process assumes that no two events will have the same movie ID in the same cycle.  You'll want to dedup the <code>eventBatch</code>.</p> <p>This process may leave orphaned records around, since the call to <code>removeOrdinalFromThisCycle()</code> doesn\u2019t remove any referenced records.  To solve this, the <code>TransitiveSetTraverser</code> can be used: <pre><code>List&lt;MovieUpdateEvent&gt; eventBatch = /// a batch of events\n\n/// find the Movie ordinals to remove\nBitSet removedMovieOrdinals = new BitSet();\nfor(MovieUpdateEvent event : eventBatch) {\n \u00a0\u00a0\u00a0int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n \u00a0\u00a0\u00a0removedMovieOrdinals.set(oldOrdinal);\n}\n\n/// initially the removal selection includes just Movies\nMap&lt;String, BitSet&gt; removeRecords = new HashMap&lt;&gt;();\nremoveRecords.put(\"Movie\", removedMovieOrdinals);\n\n/// expand the selection to include any records referenced by selected Movies\nTransitiveSetTraverser.addTransitiveMatches(readEngine, removeRecords);\n/// but don't include records which are also referenced by unselected movies\nTransitiveSetTraverser.removeReferencedOutsideClosure(readEngine, removeRecords);\n\n/// remove everything in the selection\nfor(Map.Entry&lt;String, BitSet&gt; entry : removeRecords) {\n \u00a0\u00a0\u00a0HollowTypeWriteState typeState = writeEngine.getTypeState(entry.getKey());\n \u00a0\u00a0\u00a0int removeOrdinal = entry.getValue().nextSetBit(0);\n \u00a0\u00a0\u00a0while(removeOrdinal != -1) {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0typeState.removeOrdinalFromThisCycle(removeOrdinal);\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0removeOrdinal = entry.getValue().nextSetBit(removeOrdinal + 1);\n \u00a0\u00a0\u00a0}\n}\n</code></pre></p> <p>In the above code, each of the <code>Movie</code> ordinals to be replaced are added to a <code>BitSet</code>.  Then, the <code>TransitiveSetTraverser</code> is used to expand the collection of selected records by adding any records referenced by the selected <code>Movies</code>.  Then, the <code>TransitiveSetTraverser</code> is again used to deselect any child records which are also referenced by other Movies which were not selected for removal.  Finally, the selection is actually removed from the <code>HollowWriteStateEngine</code>.</p>"},{"location":"advanced-topics/#in-memory-data-layout","title":"In-Memory Data Layout","text":"<p>Each record in Hollow begins with a fixed-length number of bits.  At the lowest level, these bits are held in long arrays using the class <code>FixedLengthElementArray</code>.  This class allows for storage and retrieval of fixed-length data in a range of bits.  For example, if a <code>FixedLengthElementArray</code> was queried for the 6-bit value starting at bit 7 in the following example range of bits:</p> <p></p> <p>The value <code>100100</code> in binary, or <code>36</code> in base 10, would be returned.</p>"},{"location":"advanced-topics/#object-layout","title":"Object Layout","text":"<p>An <code>OBJECT</code> record is a fixed set of strongly typed fields.  Each field is represented by a fixed-length number of bits.  Each record is represented by a fixed length number of bits equal to the sum of the bits required to represent each fields.  For each type, all fields of all records are packed into a single <code>FixedLengthElementArray</code>.  No bookkeeping data structures are required to locate a record -- the start bit for each record can is located by simply multiplying the number of bits per record times the record\u2019s ordinal.</p> <p></p> <p>The number of bits used to represent a field which is one of the types (<code>INT</code>, <code>LONG</code>, <code>REFERENCE</code>) is exactly equal to the number of bits required to represent the maximum value contained in the field across all records.  The values for <code>INT</code> and <code>LONG</code> fields are represented using zig-zag encoding, so that smaller absolute values require fewer bits.  The values for <code>REFERENCE</code> fields are encoded as the referenced record\u2019s ordinal, which along with the referenced type (from the schema) is sufficient to identify and locate the referenced record.</p> <p>32 bits are used to represent a <code>FLOAT</code>, and 64 bits are used to represent a <code>DOUBLE</code>.</p> <p><code>STRING</code> and <code>BYTES</code> fields each get a separate byte array, into which the values for all records are packed.  The fixed-length value in these fields are offsets into the field\u2019s byte array where the record\u2019s value ends.  In order to determine the begin byte for the record with ordinal n, the offset encoded into the record with ordinal (n-1) is read.  The number of fixed length bits used to represent the offsets is exactly equal to the number of bits required to represent the maximum offset, plus one.</p> <p>Each field type may be assigned a null value.  For <code>INT</code>, <code>LONG</code>, and <code>REFERENCE</code> fields, null is encoded as a value with all ones.  For <code>FLOAT</code> and <code>DOUBLE</code> fields, null is encoded as special bit sequences.  For <code>STRING</code> and <code>BYTES</code> fields, null is encoded by setting a designated null bit at the beginning of each field, followed by the end offset of the last populated value for that field.</p>"},{"location":"advanced-topics/#list-layout","title":"List Layout","text":"<p>A <code>LIST</code> is an ordered collection of records of a specific type.  <code>LIST</code> types are represented with two FixedLengthElementArrays.  We can refer to these arrays as the offset array and the element array.</p> <p>Each <code>LIST</code> type contains a single element array into which the references to elements for all records are packed.  References are encoded as the ordinals of the element records, which is sufficient to identify and locate the record.  Each reference is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  Each record is represented with a contiguous range of elements in the element array.  </p> <p>The offset array contains fixed-length offsets into the element array where the record\u2019s elements end.  In order to determine the begin element for the record with ordinal n, the end value for the element (n-1) is read.  </p> <p></p> <p>Elements in a <code>LIST</code> record may not be null.</p>"},{"location":"advanced-topics/#set-layout","title":"Set Layout","text":"<p>A <code>SET</code> is an unordered collection of records of a specific type.  The records for <code>SET</code> elements are hashed into an open-addressed hash table.  <code>SET</code> types are represented with two <code>FixedLengthElementArrays</code>.  We can refer to these arrays as the offset array and the bucket array.</p> <p>Each <code>SET</code> element contains a single bucket array into which the references to elements for all records are packed.  Each record is represented with a contiguous range of buckets in the bucket array.  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all elements for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.</p> <p>The offset array contains two fixed-length fields per record:  the size of the set, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.</p> <p></p> <p>Elements in a <code>SET</code> record may not be null.</p>"},{"location":"advanced-topics/#map-layout","title":"Map Layout","text":"<p>A <code>MAP</code> is an unordered collection of key/value pairs, where each key is a specific type, and each value type is a specific type.  The records for MAP elements are hashed into an open-addressed hash table.  <code>MAP</code> types are represented with two <code>FixedLengthElementArrays</code>.  We can refer to these arrays as the offset array and the bucket array.</p> <p>Each <code>MAP</code> type contains a single bucket array into which the references to keys and values for all records are packed.  Each record is represented with a contiguous range of buckets in the bucket array.  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all key/value pairs for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced key ordinal, plus the number of bits required to represent the maximum referenced value ordinal.  A populated bucket contains two fixed length fields: the first field contains the ordinal of the referenced key, and the second field contains the ordinal of the referenced value.  Empty buckets are represented with a key field containing a reserved sentinel value equal to all ones in binary.</p> <p>The offset array contains two fixed-length fields per record:  the size of the map, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.</p> <p></p> <p>A <code>MAP</code> cannot contain null keys or values.</p>"},{"location":"advanced-topics/#primary-key-index-layout","title":"Primary Key Index Layout","text":"<p>A primary key index is a single <code>FixedLengthElementArray</code>, which represents an open-addressed hash table of pointers to records of the given type.  The hash of each record is derived based on the fields designated in the primary key.  Each bucket in the hash table is represented using a fixed number of bits equal to the number of bits required to represent the maximum ordinal of the indexed type.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.</p> <p>When queried with a key, the index will hash the key, look in the corresponding bucket to find the ordinal of a record which also hashes to this key, then compare the referenced record\u2019s key to the query.  When a matching record is found, the ordinal at that bucket is returned as the match.  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered, no such record exists in the dataset.</p>"},{"location":"advanced-topics/#hash-index-layout","title":"Hash Index Layout","text":"<p>A hash index is uses two <code>FixedLengthElementArrays</code>.  These arrays can be referred to as the match array and the select array.  The match array is an open-addressed hash table and contains buckets.  Like a primary key index, the match array does not re-encode the values of keys.  Instead, it retains pointers to existing records which may be used to retrieve the hashed keys.</p> <p>Unlike a primary key index, it may not be sufficient to retain a single pointer to the indexed type and extract the hashed key from that record -- each record in a hash index may match multiple keys.  Instead, we retain 1-n pointers.  Each pointer will indicate a record through which one or more of the key fields may be unambiguously retrieved.  If multiple key fields may be unambiguously traversed to via a single type, then only a single pointer for the field group will be retained per bucket.  </p> <p>Consider a scenario in which a hash index is used to index Movies by the nationalities and birth year of actors.  For the key <code>[\u201cBritish\u201d, 1972]</code>, the corresponding entry in the match array may contain a pointer to the <code>Actor</code> record for Idris Elba.  Although the record points to a specific <code>Actor</code>, the matching records for this entry will contain movies starring any British Actor born in 1972.</p> <p>Each pointer field in the match array bucket references a specific type, and is encoded as the ordinal to which the bucket refers.  Each is represented using a fixed number of bits equal to the maximum ordinal in the referenced type.</p> <p>In addition to pointers which allow us to look up the matching key, each bucket in the match array includes the number of matching records, and an offset into the select array.  The select array contains lists of ordinals to matching records.</p> <p>When queried with a key, the index will hash the key, then look to the corresponding bucket in the match array.  The match pointers are used to compare the queried key with the matching key.  If a match is found, then the corresponding entries in the select array are returned as a <code>HollowHashIndexResult</code>.  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered in the match array, no such record exists in the dataset.</p>"},{"location":"advanced-topics/#shared-memory-mode","title":"Shared memory mode","text":"<p>Traditionally, an entire Hollow dataset is loaded in the JVM heap. While this approach has its advantages, it also imposes eager loading of the underlying data and limits the data size to size of available physical memory. An alternative approach is to use memory mapping to map Hollow data to virtual memory and then eagerly or lazily load data into off-heap physical memory. Eager loading would memory lock the dataset and provide similar performance guarantee as traditional on-heap Hollow. Lazy loading would defer loading data to physical memory to when data is accessed (page fault would be incurred which would load 4k sized pages to physical memory, hot data would be retained in physical memory) thereby enabling faster application initialization and support for TB-scale datasets. Mapping Hollow data to shared memory also allows for memory deduplication across Hollow consumers on the same machine.</p> <p>The shared memory implementation is largely future work, but a limited shared-memory based lazy load functionality has been implemented. When configured for shared memory mode, a consumer will perform an initial snapshot load, it will not apply delta transitions, and data structures tracking indices live on-heap. This limited functionality can be useful for local debugging with large Hollow datasets.</p>"},{"location":"community/","title":"Getting Support","text":"<p>For bug reports and feature requests, please file a GitHub issue.  </p> <p>If you have a question that isn't covered in this documentation, please reach out for help either on Stack Overflow or Gitter</p>"},{"location":"community/#stack-overflow","title":"Stack Overflow","text":"<p>We monitor posts tagged with <code>hollow</code>.</p>"},{"location":"community/#gitter","title":"Gitter","text":"<p>We are often available for chat via Gitter.  We hope that you'll stick around and pay it forward by answering other users' questions when they arise.</p>"},{"location":"community/#contributing-to-hollow","title":"Contributing to Hollow","text":"<p>We'll gladly review and accept pull requests for Hollow.  If you want to have a design discussion for your changes, please reach out to us on Gitter.  There is a fake dataset of reasonable size and complexity available for testing against, please see the section on Fake dataset. </p>"},{"location":"community/#backwards-compatibility","title":"Backwards Compatibility","text":"<p>New features in Hollow should always be added in a way that is backwards compatible, except in extremely rare cases when a major version is released.</p> <p>If you would like to make a contribution which breaks backwards compatibility, please contact us so we can evaluate alternate ways to achieve the desired result, and/or whether to schedule the change for an upcoming major version release.</p>"},{"location":"community/#dependencies","title":"Dependencies","text":"<p>The core project hollow should have zero <code>compile</code> dependencies, and should only depend on one library (jUnit) as a <code>test</code> dependency.  We believe this provides long-term stability for users, reduces licensing concerns, and eliminates the possibility that other project dependencies will be compiled against incompatible versions of dependent libraries.</p> <p>If you would like to make a contribution which requires a third-party dependency, please contact us before proceeding so we can discuss the appropriate location for the addition.</p>"},{"location":"community/#sunmiscunsafe","title":"sun.misc.Unsafe","text":"<p>The core project hollow utilizes <code>sun.misc.Unsafe</code>. Your IDE may treat this as an error. See Issue #5 for how to compile without errors.</p>"},{"location":"data-ingestion/","title":"Data Ingestion","text":"<p>Hollow includes a few ready-made data ingestion mechanisms.  Additionally, custom data ingestion mechanisms can be created relatively easily using the Low Level Input API.</p>"},{"location":"data-ingestion/#hollowobjectmapper","title":"HollowObjectMapper","text":"<p>When using a <code>HollowProducer</code>, each call to <code>state.add(obj)</code> is delegated to a <code>HollowObjectMapper</code>.  The <code>HollowObjectMapper</code> is used to add POJOs into a <code>HollowWriteStateEngine</code>:</p> <pre><code>HollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    writeEngine.add(movie);\n</code></pre> <p>The <code>HollowObjectMapper</code> can also be used to initialize the data model of a <code>HollowWriteStateEngine</code> without adding any actual data: <pre><code>HollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nmapper.initializeTypeState(Movie.class);\nmapper.initializeTypeState(Show.class);\n</code></pre></p> <p>Schemas will be assumed based on the field and type names in the POJOs.  Any referenced types will also be traversed and included in the derived data model.</p> <p>Thread Safety</p> <p>The <code>HollowObjectMapper</code> is thread-safe; multiple threads may add Objects at the same time.</p>"},{"location":"data-ingestion/#specifying-type-names-in-the-hollowobjectmapper","title":"Specifying type names in the HollowObjectMapper","text":"<p>By default, type names are equal to the names of classes added to the <code>HollowObjectMapper</code>.  Alternatively, the name of a type may be explicitly defined by using the <code>@HollowTypeName</code> annotation.  This annotation can be added at either the class or field level.</p> <p>The following example <code>Award</code> class will reference a type <code>AwardName</code>, which will contain a single string value: <pre><code>public class Award {\n    long id;\n\n    @HollowTypeName(name=\"AwardName\")\n    String name;\n}\n</code></pre></p> <p>The following example <code>Category</code> class will be added as the type <code>Genre</code>, where not otherwise specified by referencing fields: <pre><code>@HollowTypeName(name=\"Genre\")\npublic class Category {\n   ...\n}\n</code></pre></p> <p>Namespaced fields</p> <p>Using the <code>@HollowTypeName</code> attribute is a convenient way to add appropriate record type namespacing into your data model.</p>"},{"location":"data-ingestion/#inlining-fields-in-the-hollowobjectmapper","title":"Inlining fields in the HollowObjectMapper","text":"<p>You can inline fields in the <code>HollowObjectMapper</code> by annotating them with <code>@HollowInline</code>.  The following example <code>Creator</code> class inlines the field <code>creatorName</code>:</p> <pre><code>public class Creator {\n    long id;\n\n    @HollowInline\n    String creatorName;\n}\n</code></pre> <p>The following <code>java.lang.*</code> types can be inlined:</p> <ul> <li><code>String</code></li> <li><code>Boolean</code></li> <li><code>Integer</code></li> <li><code>Long</code></li> <li><code>Double</code></li> <li><code>Float</code></li> <li><code>Short</code></li> <li><code>Byte</code></li> <li><code>Character</code></li> </ul>"},{"location":"data-ingestion/#memoizing-pojos-in-the-hollowobjectmapper","title":"Memoizing POJOs in the HollowObjectMapper","text":"<p>If a long field named <code>__assigned_ordinal</code> is defined in a POJO class, then <code>HollowObjectMapper</code> will use this field to record the assigned ordinal when Objects of this class are added to the state engine.  </p> <p>When the <code>HollowObjectMapper</code> sees this POJO again, it will short-circuit writing to the state engine and discovering or assigning an ordinal -- it will instead return the previously recorded ordinal.  If during processing you can reuse duplicate referenced POJOs, then you can use this effect to greatly speed up adding records to the state engine.</p> <p>If the <code>__assigned_ordinal</code> field is present, it should be initialized to <code>HollowConstants.ORDINAL_NONE</code>.  The field may be (but does not have to be) private and/or final.</p> <p>The following example <code>Director</code> class uses the <code>__assigned_ordinal</code> optimization: <pre><code>public class Director {\n    long id;\n    String directorName;\n\n    private transient final long __assigned_ordinal = HollowConstants.ORDINAL_NONE;\n}\n</code></pre></p> <p>Warning</p> <p>If the <code>__assigned_ordinal</code> optimization is used, POJOs should not be modified after they are added to the state engine.  Any modifications after the first time a memoized POJO is added to the state engine will be ignored and any references to these POJOs will always point to the originally added record. Using immutable POJOs can help reduce errors.</p>"},{"location":"data-ingestion/#json-to-hollow","title":"JSON to Hollow","text":"<p>The project hollow-jsonadapter contains a component which will automatically parse json into a <code>HollowWriteStateEngine</code>.  The expected format of the json will be defined by the schemas in the <code>HollowWriteStateEngine</code>.  The data model must be pre-initialized.  See the Schema Parser topic in this document for an easy way to configure the schemas with a text document.</p> <p>The <code>HollowJsonAdapter</code> class is used to populate records of a single type from a json file.  A single record: <pre><code>{\n  \"id\": 1,\n  \"releaseYear\": 1999,\n  \"actors\": [\n     {\n        \"id\": 101,\n        \"actorName\": \"Keanu Reeves\"\n     },\n     {\n        \"id\": 102,\n        \"actorName\": \"Laurence Fishburne\"\n     }\n  ]\n}\n</code></pre></p> <p>Can be parsed with the following code: <pre><code>String json = /// the record above\n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine, \"Movie\");\n\njsonAdapter.processRecord(json);\n</code></pre></p> <p>If a field defined in the schema is not encountered in the json data, the value will be null in the corresponding Hollow record.  If a field is encountered in the json data which is not defined in the schema, the field will be ignored.</p> <p>A large number of records in a single file can also be processed: <pre><code>Reader reader = /// a reader for the json file\n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine, \"Movie\");\n\njsonAdapter.populate(reader);\n</code></pre></p> <p>When processing an entire file, it is expected that the file contains only a single json array of records of the expected type.  The records will be processed in parallel.</p> <p>Hollow to JSON</p> <p>Hollow objects can be converted to JSON string using <code>HollowRecordJsonStringifier</code>. Tools backed by Hollow data is one of the cases where this can be useful.</p>"},{"location":"data-ingestion/#zeno-to-hollow","title":"Zeno to Hollow","text":"<p>The project hollow-zenoadapter has an adapter which can be used with Hollow\u2019s predecessor, Zeno.  We used this as part of our migration path from Zeno to Hollow, and it is provided for current users of Zeno who would like to migrate to Hollow as well.  Start with the HollowStateEngineCreator.</p>"},{"location":"data-modeling/","title":"Data Modeling","text":"<p>This section describes details on schemas used in Hollow and how your data model maps to them.</p>"},{"location":"data-modeling/#schemas","title":"Schemas","text":"<p>A Hollow data model is a set of schemas, which are usually defined by the POJOs used on the producer to populate the data.  This section will use POJOs as examples, but there are other ways to define schemas -- for example you could ingest a text file and use the schema parser.</p> <p>Schemas Define the Data Model</p> <p>A hollow dataset is comprised of one or more data types.  The data model for a dataset is defined by the schemas describing those types.</p>"},{"location":"data-modeling/#object-schemas","title":"Object Schemas","text":"<p>Each POJO class you define will result in an <code>Object</code> schema, which is a fixed set of strongly typed fields.  The fields will be based on the member variables in the class.  For example, the class <code>Movie</code> will define an <code>Object</code> schema with three fields:</p> <pre><code>public class Movie {\n    int movieId;\n    String title;\n    Set&lt;Actor&gt; actors;\n}\n</code></pre> <p>Each schema has a type name.  The name of the type will default to the simple name of your POJO -- in this case <code>Movie</code>.  </p> <p>Each schema field has a field name, which will default to the same name as the field in the POJO -- in this case <code>movieId</code>, <code>title</code>, and <code>actors</code>.  Each field also has a field type, which is in this case <code>INT</code>, <code>REFERENCE</code>, and <code>REFERENCE</code>, respectively.  Each <code>REFERENCE</code> field also indicates the referenced type, which for our reference fields above default to <code>String</code> and <code>SetOfActor</code>.</p> <p>The possible field types are:</p> <ul> <li><code>INT</code>: An integer value up to 32-bits. Integer.MIN_VALUE is reserved for a sentinel value indicating null.</li> <li><code>LONG</code>: An integer value up to 64-bits. Long.MIN_VALUE is reserved for a sentinel value indicating null.</li> <li><code>FLOAT</code>: A 32-bit floating-point value</li> <li><code>DOUBLE</code>: A 64-bit floating-point value</li> <li><code>BOOLEAN</code>: <code>true</code> or <code>false</code></li> <li><code>STRING</code>: An array of characters</li> <li><code>BYTES</code>: An array of bytes</li> <li><code>REFERENCE</code>: A reference to another specific type.  The referenced type must be defined by the schema.</li> </ul> <p>Notice that since the reference type is defined by the schema, data models must be strongly typed.  Each reference in your data model must point to a specific concrete implementation.  References to interfaces, abstract classes, or <code>java.lang.Object</code> are not supported.</p>"},{"location":"data-modeling/#primary-keys","title":"Primary Keys","text":"<p><code>Object</code> schemas may specify a primary key.  This is accomplished by using the <code>@HollowPrimaryKey</code> annotation and specifying the fields.</p> <pre><code>@HollowPrimaryKey(fields={\"movieId\"})\npublic class Movie {\n    int movieId;\n    String title;\n    Set&lt;Actor&gt; actors;\n}\n</code></pre> <p>When defined in the schema, primary keys are a part of your data model and drive useful functionality and default configuration in the hollow explorer, hollow history, and diff ui.  They also provide a shortcut when creating a primary key index.</p> <p>Primary keys defined in the schema follow the same convention as primary keys defined for indexes.  They consist of one or more field paths, which will auto-expand if they terminate in a <code>REFERENCE</code> field.</p> <p>Duplidate record validation</p> <p>Hollow producer does not dedupe records based on primary key definition, by default. For e.g. if two records with the same value for primary key field(s) but different values for other field(s) are added to the write state in a runCycle, both records will be accepted. A consumer querying using primary key will see inconsistent behavior. Either one of those records can be returned. To address this, Hollow offers the DuplicateDataDetectionValidator, which enforces primary key constraints. This validator is not enabled by default and must be opted into.</p> <p>Null values are not supported</p> <p>Primary key field(s) cannot have null values. This is not supported as it was not needed. Please be mindful when adding values to primary key fields. This will result in exception similar to below. </p> <pre><code>java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Attempting to read null value as int\nat com.netflix.hollow.core.write.HollowBlobWriter.writeSnapshot(HollowBlobWriter.java:69)\nat com.netflix.hollow.api.producer.fs.HollowFilesystemBlobStager$FilesystemBlob\n.write(HollowFilesystemBlobStager.java:117)\n</code></pre>"},{"location":"data-modeling/#inlined-vs-referenced-fields","title":"Inlined vs Referenced Fields","text":"<p>We can inline some fields in our POJOs so that they are no longer <code>REFERENCE</code> fields, but instead encode their data directly in each record:</p> <p><pre><code>public class Movie {\n    int movieId;\n    @HollowInline String title;\n    Set&lt;Actor&gt; actors;\n}\n</code></pre> In the above example, our fields are now of type <code>INT</code>, <code>STRING</code>, and <code>REFERENCE</code>.</p> <p>While modeling data, we choose whether or not to inline a field for efficiency.  Consider the following type:</p> <pre><code>public class Award {\n    String awardName;\n    long movieId;\n    long actorId;\n}\n</code></pre> <p>In this case, imagine <code>awardName</code> is something like \u201cBest Supporting Actress\u201d.  Over the years, many such awards will be given, so we\u2019ll have a lot of records which share that value.  If we use an inlined <code>STRING</code> field, then the value \"Best Supporting Actress\" will be repeated for every such award record.  However, if we reference a separate record type, all such awards will reference the same child record with that value.  If the <code>awardName</code> values have a lot of repetition, then this can result in a significant savings.</p> <p>Deduplication</p> <p>Record deduplication happens automatically at the record granularity in Hollow.  Try to model your data such that when there is a lot of repetition in records, the repetitive fields are encapsulated into their own types.</p> <p>To consider the opposite case, let\u2019s examine the following <code>Actor</code> type: <pre><code>public class Actor {\n    long id;\n    @HollowInline String actorName;\n}\n</code></pre></p> <p>The <code>actorName</code> is unlikely to be repeated often.  In this case, if we reference a separate record type, we have to retain roughly the same number of unique character strings plus we need to retain references to those records.  In this case, we end up saving space by using an inlined <code>STRING</code> field instead of a reference to a separate type.</p> <p>Reference Costs</p> <p>A <code>REFERENCE</code> field isn't free, and therefore we shouldn't necessarily try to encapsulate fields inside their own record types where we won't benefit from deduplication.  These fields should instead be inlined.</p> <p>We refer to fields which are defined with native Hollow types as inlined fields, and fields which are defined as references to types with a single field as referenced fields.</p>"},{"location":"data-modeling/#namespaced-record-type-names","title":"Namespaced Record Type Names","text":"<p>In order to be very efficient, referenced types sometimes should be namespaced so that fields with like values may reference the same record type, but reference fields of the same primitive type elsewhere in the data model use different record types.  For example, consider our <code>Award</code> type again, but this time, we\u2019ll reference a type called <code>AwardName</code>, instead of <code>String</code>.  We can explicitly name the type of a field with the <code>@HollowTypeName</code> annotation: <pre><code>public class Award {\n    @HollowTypeName(name=\"AwardName\")\n    String awardName;\n    long movieId;\n    long actorId;\n}\n</code></pre></p> <p>Other types in our data model which reference award names can reuse the <code>AwardName</code> type.  Other referenced string fields in our data model, which are unrelated to award names, should use different types corresponding to the semantics of their values.  </p> <p>Namespacing fields saves space because references to types with a lower cardinality use fewer bits than references to types with a higher cardinality.  The reason for this can be gleaned from the In-Memory Data Layout topic underneath the Advanced Topics section.</p> <p>Namespacing fields is also useful if some consumers don't need the contents of a specific referenced field.  If a type is namespaced, it can be selectively filtered, whereas if it is grouped with other fields which are needed by all consumers, then it cannot be selected for filtering.</p> <p>Namespacing Reduces Reference Costs</p> <p>Using an appropriately namespaced type reduces the heap footprint cost of <code>REFERENCE</code> fields.</p> <p>Changing default type names</p> <p>The <code>@HollowTypeName</code> annotation can also be used at the class level to select a default type name for a class other than its simple name. Custom type names should begin with an upper case character to avoid ambiguity in naming in the generated API, although this is not enforced by Hollow due to backwards compatibility reasons.</p>"},{"location":"data-modeling/#grouping-associated-fields","title":"Grouping Associated Fields","text":"<p>Referencing fields can save space because the same field values do not have to be repeated for every record in which they occur.  Similarly, we can group fields which have covarying values, and pull these out from larger objects as their own types.  For example, imagine we started with a <code>Movie</code> type which included the following fields: <pre><code>public class Movie {\n    long id;\n    String title;\n    String maturityRating;\n    String advisories;\n}\n</code></pre></p> <p>We might notice that the <code>maturityRating</code> and <code>advisories</code> fields vary together, and are often the repeated across many <code>Movie</code> records.  We can pull out a separate type for these fields: <pre><code>public class Movie {\n    long id;\n    String title;\n    MaturityRating maturityRating;\n}\n\npublic class MaturityRating {\n    string rating;\n    string advisories;\n}\n</code></pre></p> <p>We could have referenced these fields separately.  If we had done so, each <code>Movie</code> record, of which there are probably many, would have had to contain two separate references for these fields.  Instead, by recognizing that these fields were associated and pulling them together, space is saved because each <code>Movie</code> record now only contains one reference for this data.</p>"},{"location":"data-modeling/#transient-fields","title":"Transient Fields","text":"<p>A transient field is ignored and will not be included in an <code>Object</code> Schema.  A transient field is a field declared with the <code>transient</code> Java keyword or annotated with the <code>@HollowTransient</code> annotation.  The latter may be used for cases when the use of the <code>transient</code> Java keyword has unwanted side-effects, such as when POJOs defining the data  model are also consumed by tools, other than Hollow, for which the field is not transient.</p>"},{"location":"data-modeling/#list-schemas","title":"List Schemas","text":"<p>You can define <code>List</code> schemas by adding a member variable of type <code>List</code> in your data model.  For example:</p> <pre><code>public class Movie {\n    long id;\n    String title;\n    List&lt;Award&gt; awardsReceived;\n}\n</code></pre> <p>The <code>List</code> must explicitly define its parameterized element type.  The default type name of the above <code>List</code> schema will be <code>ListOfAward</code>.  </p> <p>A <code>List</code> schema indicates a record type which is an ordered collection of <code>REFERENCE</code> fields.  Each record will have a variable number of references.  The referenced type must be defined by the schema, and all references in all records will encode only the ordinals of the referenced records as the values for these references.</p>"},{"location":"data-modeling/#set-schemas","title":"Set Schemas","text":"<p>You can define <code>Set</code> schemas by adding a member variable of type <code>Set</code> in your data model.  The <code>Set</code> must explicitly define its parameterized element type.  For example:</p> <pre><code>public class Movie {\n    long id;\n    String title;\n\n    @HollowHashKey(fields={\"firstName\", \"lastName\"}) /// hash key is optional\n    Set&lt;Actor&gt; cast;\n}\n</code></pre> <p>A <code>Set</code> schema indicates a record type which is an unordered collection of <code>REFERENCE</code> fields.  Each record will have a variable number of references, and the referenced type must be defined by the schema.  Within a single set record, each reference must be unique.  </p> <p>References in <code>Set</code> records can be hashed by some specific element fields for O(1) retrieval.  In order to enable this feature, a <code>Set</code> schema will define an optional hash key, which defines how its elements are hashed/indexed.</p>"},{"location":"data-modeling/#map-schemas","title":"Map Schemas","text":"<p>You can define <code>Map</code> schemas by adding a member variable of type <code>Map</code> in your data model.  The <code>Map</code> must explicitly define it parameterized key and values types.  For example:</p> <pre><code>public class Movie {\n    long id;\n    String title;\n\n    @HollowHashKey(fields=\"actorId\") /// hash key is optional\n    Map&lt;Actor, Role&gt; cast;\n}\n</code></pre> <p>A <code>Map</code> schema indicates a record type which is an unordered collection of pairs of <code>REFERENCE</code> fields, used to represent a key/value mapping.  Each record will have a variable number of key/value pairs.  Both the key reference type and the value reference type must be defined by the schema.  The key reference type does not have to be the same as the value reference type.  Within a single map record, each key reference must be unique.  </p> <p>Entries in <code>Map</code> records can be hashed by some specific key fields for O(1) retrieval of the keys, values, and/or entries.  In order to enable this feature, a <code>Map</code> schema will define an optional hash key, which defines how its entries are hashed/indexed.</p>"},{"location":"data-modeling/#hash-keys","title":"Hash Keys","text":"<p>Each <code>Map</code> and <code>Set</code> schema may optionally define a hash key.  A hash key specifies one or more user-defined fields used to hash entries into the collection.  When a hash key is defined on a <code>Set</code>, each set record becomes like a primary key index; records in the set can be efficiently retrieved by matching the specified hash key fields.  Similarly, when a hash key is defined on a <code>Map</code>, each map record becomes like an index over the keys in the key/value pairs contained in the map record.</p> <p>See Hash Keys for a detailed discussion of hash keys.</p>"},{"location":"data-modeling/#circular-references","title":"Circular References","text":"<p>Circular references are not allowed in Hollow.  A type may not reference itself, either directly or transitively.</p>"},{"location":"data-modeling/#object-memory-layout","title":"Object Memory Layout","text":"<p>On consumers, <code>INT</code> and <code>LONG</code> fields are each represented by a number of bits exactly sufficient to represent the maximum value for the field across all records.  <code>FLOAT</code>, <code>DOUBLE</code>, and <code>BOOLEAN</code> fields are represented by 32, 64, and 2 bits, respectively.  <code>STRING</code> and <code>BYTES</code> fields use a variable number of bytes for each record.  <code>REFERENCE</code> fields encode the ordinal of referenced records, and are represented by a number of bits exactly sufficient to encode the maximum ordinal of the referenced type.  See In-memory Data Layout for more details.</p> <p>Avoid Outlier Values</p> <p>Try to model your data such that there aren't any outlier values for <code>INT</code> and <code>LONG</code> fields.  Also, avoid <code>FLOAT</code> and <code>DOUBLE</code> fields where possible, since these field types are relatively expensive.</p>"},{"location":"data-modeling/#maintaining-backwards-compatibility","title":"Maintaining Backwards Compatibility","text":"<p>A data model will evolve over time.  The following operations will not impact the interoperability between existing clients and new data:</p> <ul> <li>Adding a new type</li> <li>Removing an existing type</li> <li>Adding a new field to an existing type</li> <li>Removing an existing field from an existing type.</li> </ul> <p>When adding new fields or types, existing generated client APIs will ignore the new fields, and all of the fields which existed at the time of API generation will still be visible using the same methods.  When removing fields, existing generated client APIs will see null values if the methods corresponding to the removed fields are called.  When removing types, existing generated client APIs will see removed types as having no records.</p> <p>It is not backwards compatible to change the type of an existing field.  The client behavior when calling a method corresponding to a field with a changed type is undefined.</p> <p>It is not backwards compatible to change the primary key or hash key for any type.</p> <p>Beyond the specification of Hollow itself, backwards compatibility often has a lot to do with the use case and semantics of the data. Hollow will always behave in the stated way for evolving data models, but it\u2019s possible that consumers require a field which starts returning null once it gets removed.  For this reason, additional caution should be exercised when removing types and fields.</p> <p>Backwards-incompatible data remodeling</p> <p>Every so often, it may be required or desirable to make changes to the data model which are incompatible with prior versions.  In this case, an older producer, which produces the older data model, should run in parallel with the newer producer, producing the newer incompatible data model.  Each producer should write its blobs to a different namespace, so that older consumers can read from the old data model, and newer consumers can read from the newer data model.  Once all consumers are upgraded and reading from the newer data model, the older producer can be decommissioned.</p>"},{"location":"diving-deeper/","title":"Diving Deeper","text":"<p>The following section details some lower-level concepts which will provide the backdrop for some more advanced usage of Hollow.  If we didn't have a <code>HollowProducer</code> or <code>HollowConsumer</code>, this section details how we would use Hollow.</p>"},{"location":"diving-deeper/#state-engines","title":"State Engines","text":"<p>Both the <code>HollowProducer</code> and <code>HollowConsumer</code> handle datasets with a state engine.  A state engine can be transitioned between data states.  A producer uses a write state engine and a consumer uses a read state engine.  </p> <ul> <li>A <code>HollowReadStateEngine</code> can be obtained from a <code>HollowConsumer</code> via the method <code>getStateEngine()</code>.</li> <li>A <code>HollowWriteStateEngine</code> can be obtained from a <code>HollowProducer</code> via the method <code>getWriteEngine()</code>.</li> </ul>"},{"location":"diving-deeper/#ordinals","title":"Ordinals","text":"<p>Each record in a Hollow data state is assigned to a specific ordinal, which is an integer value. An ordinal:</p> <ul> <li>is a unique identifier of the record within a type.</li> <li>is sufficient to locate the record within a type.</li> </ul> <p>Ordinals are automatically assigned by Hollow. They lie in the range of 0-n, where n is generally not much larger than the total number of records for the type.  In lower-level usage of Hollow, ordinals are often used as proxies for handles to specific records.</p> <p>Given a <code>HollowReadStateEngine</code>, you can retrieve the set of currently populated ordinals using the call <code>stateEngine.getTypeState(\"TypeName\").getPopulatedOrdinals()</code>.  A <code>BitSet</code> containing all of the populated ordinals is returned.  Similarly, the ordinals which were populated prior to the last delta transition can be obtained using <code>stateEngine.getTypeState(\"TypeName\").getPreviousOrdinals()</code>.</p> <p>Populated Ordinals</p> <p>Never modify the <code>BitSet</code> returned from <code>getPopulatedOrdinals()</code> or <code>getPreviousOrdinals()</code>.  Modifying these may corrupt the data store.</p> <p>It's useful to note that records in Hollow are immutable.  They will never be modified, only removed and added.  A modification probably means that within the same delta there was a removal of a record keyed by some value and an addition of a new record keyed by the same value.</p> <p>Ordinals have some useful properties:</p> <ul> <li>It is guaranteed that if an exactly equivalent record exists in two adjacent states, then that record will retain the same ordinal. If, on the other hand, a record does not have an exact equivalent in an adjacent state, then its ordinal will not be populated in the state in which it does not exist.</li> <li>After a single delta transition has been applied which removes a record, that record will be marked as not populated, but the data for that record will still be accessible at that ordinal until the next delta transition.  We call these records ghost records.</li> </ul>"},{"location":"diving-deeper/#writing-a-data-snapshot","title":"Writing a Data Snapshot","text":"<p>Let's assume we have a POJO class <code>Movie</code>: <pre><code>public class Movie {\n    long id;\n    String title;\n    int releaseYear;\n\n    public Movie(long id, String title, int releaseYear) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = releaseYear;\n    }\n}\n</code></pre></p> <p>In order to create a new data state and write it to disk, we can use a <code>HollowWriteStateEngine</code> directly: <pre><code>HollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    mapper.add(movie);\n\nOutputStream os = ...; /// where to write the blob\nHollowBlobWriter writer = new HollowBlobWriter(writeEngine);\nwriter.writeSnapshot(os);\n</code></pre></p> <p>A <code>HollowWriteStateEngine</code> is the main handle to a Hollow dataset for a data producer.  A <code>HollowObjectMapper</code> is one of a few different ways to populate a <code>HollowWriteStateEngine</code> with data.  When starting with POJOs, it's the easiest way.</p> <p>We'll use a <code>HollowBlobWriter</code> to write the current state of a <code>HollowWriteStateEngine</code> to an <code>OutputStream</code>.  We call the data which gets written to the <code>OutputStream</code> a blob.  </p>"},{"location":"diving-deeper/#reading-a-data-snapshot","title":"Reading a Data Snapshot","text":"<p>A data consumer can load a snapshot created by the producer into memory: <pre><code>HollowReadStateEngine readEngine = new HollowReadStateEngine();\nHollowBlobReader reader = new HollowBlobReader(readEngine);\n\nInputStream is = /// where to load the snapshot from\nreader.readSnapshot(is);\n</code></pre></p> <p>A <code>HollowReadStateEngine</code> is our main handle to a Hollow dataset as a consumer.  A <code>HollowBlobReader</code> is used to consume blobs into a <code>HollowReadStateEngine</code>.  Above, we're consuming a snapshot blob in order to initialize our state engine.  </p> <p>Once this dataset is loaded into memory, we can access the data for any records using our generated API: <pre><code>MovieAPI movieApi = new MovieAPI(readEngine);\n\nfor(Movie movie : movieApi.getAllMovieHollow()) {\n    /// do something for each Movie record\n}\n</code></pre></p>"},{"location":"diving-deeper/#writing-a-delta","title":"Writing a Delta","text":"<p>Some time has passed and the dataset has evolved.  The producer, with the same <code>HollowWriteStateEngine</code> in memory, needs to communicate this updated dataset to consumers.  The data for the new state must be added to the state engine, after which a transition from the previous state to the new state can be written as a delta blob: <pre><code>writeEngine.prepareForNextCycle();\n\nfor(Movie movie : movies)\n    mapper.add(movie);\n\nOutputStream os = ....; /// where to write the delta blob\nwriter.writeDelta(os);\n</code></pre></p> <p>Let's take a closer look at what the above code does.  The same <code>HollowWriteStateEngine</code> which was used to produce the snapshot blob is used -- it already knows everything about the prior state and can be transitioned to the next state.  We call <code>prepareForNextCycle()</code> to inform the state engine that the writing of blobs from the prior state is complete, and populating data into the next state is about to begin.  When creating a new state, all of the movies currently in our dataset are re-added again.  It's not necessary to figure out which records were added, removed, or modified -- that's Hollow's job.</p> <p>We can (but don't have to) use the same <code>HollowObjectMapper</code> and/or <code>HollowBlobWriter</code> as we used in the prior cycle to create the initial snapshot.  </p> <p>The call to <code>writeDelta()</code> records a delta blob to the <code>OutputStream</code>.  Encoded into the delta is a set of instructions to update a consumer\u2019s read state engine from the previous state to the current state.</p> <p>Reverse Deltas</p> <p>Just as you can call <code>writeDelta()</code> to write a delta from one state to the next, you can also call <code>writeReverseDelta()</code> to write the reverse operation which will take you from the next state to the prior state.</p>"},{"location":"diving-deeper/#reading-a-delta","title":"Reading a Delta","text":"<p>Once a delta is available the HollowReadStateEngine can be updated on the client: <pre><code>InputStream is = /// where to load the delta from\nHollowBlobReader reader = new HollowBlobReader(readEngine);\nreader.applyDelta(is);\n</code></pre></p> <p>The same <code>HollowReadStateEngine</code> into which our snapshot was consumed must be reused to consume a delta blob.  This state engine knows everything about the current state and can use the instructions in a delta to transition to the next state.  We can (but don't have to) reuse the same <code>HollowBlobReader</code>.</p> <p>After this delta has been applied, the read state engine is at the new state.  </p> <p>Thread Safety</p> <p>It is safe to use the HollowReadStateEngine to retrieve data while a delta transition is in progress.</p> <p>Delta Mismatch</p> <p>If a delta application is attempted onto a <code>HollowReadStateEngine</code> which is at a state from which the delta did not originate, then an exception is thrown and the state engine remains safely unchanged.</p>"},{"location":"diving-deeper/#indexing-data-for-retrieval","title":"Indexing Data for Retrieval","text":"<p>In prior examples the generated Hollow API was used by the data consumer to iterate over all <code>Movie</code> records in the dataset.  Most often, however, it isn\u2019t desirable to iterate over the entire dataset \u2014 instead, specific records will be accessed based on some known key.  Let\u2019s assume that the <code>Movie</code>\u2019s id is a known key.</p> <p>After consumers have populated a <code>HollowReadStateEngine</code>, the data can be indexed: <pre><code>HollowPrimaryKeyIndex idx =\n                      new HollowPrimaryKeyIndex(readEngine, \"Movie\", \"id\");\n\nidx.listenForDeltaUpdates(); // but does not listen for double-snapshot, see section on \"Keeping an index up-to-date\"\n</code></pre></p> <p>This index can be held in memory and then used in conjunction with the generated Hollow API to retrieve Movie records by id: <pre><code>int movieOrdinal = idx.getMatchingOrdinal(2);\nif(movieOrdinal != -1) {\n    MovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n    System.out.println(\"Found Movie: \" + movie._getTitle()._getValue());\n}\n</code></pre></p> <p>Which outputs: <pre><code>Found Movie: Beasts of No Nation\n</code></pre></p> <p>Thread Safety</p> <p>Retrievals from a <code>HollowPrimaryKeyIndex</code> are thread-safe.  It is safe to use a <code>HollowPrimaryKeyIndex</code> from multiple threads,  and it is safe to query while a transition is in progress.</p> <p>Ordinals</p> <p>See ordinals for a discussion about ordinals.</p>"},{"location":"diving-deeper/#keeping-an-index-up-to-date","title":"Keeping an index up-to-date","text":"<p>Indexes by default do not recalculate their state when the consumer refreshes to a new data state.  As such results for queries may become stale or corrupt if the consumer is refreshed.</p> <p>The index classes in the generated API (for e.g. <code>MoviePrimaryKeyIndex</code> or <code>&lt;API classname&gt;HashIndex</code>) have public constructors  that accept a boolean <code>isListenToDataRefresh</code> argument for whether or not to keep the indexes up to date.</p> <p>For indexes created using higher-level type-safe index API: <code>UniqueKeyIndex</code>, <code>HashIndex</code>, and <code>HashIndexSelect</code>: to subscribe these to consumer refreshes the caller must add the index as a refresh listener on the consumer. If/when the index is no longer required the index should be removed as a listener.</p> <p>For the lower-level index API: <code>HollowPrimaryKeyIndex</code>, <code>HollowHashIndex</code>, and <code>HollowUniqueKeyIndex</code> (latter is similar to <code>HollowPrimaryKeyIndex</code> but also supports object longevity) additional consideration is required for keeping an index up to date. The caller is required to call <code>listenForDeltaUpdates()</code> to subscribe an index to updates and if/when an index is to be cleaned up the caller must call <code>detachFromDeltaUpdates()</code> on the index. Just doing that is sufficient for consumers that are configured to only allow delta transitions i.e. allows delta updates but does not allow double snapshot updates. The default configuration for a Hollow Consumer is to allow double snapshot updates so in most cases with the lower-level index API calling <code>listenForDeltaUpdates()</code> is not sufficient- it can leave the index is a stale or corrupt state if a double snapshot is incurred. For handling double-snapshots on the consumer, the caller must also attach another listener on the consumer that listens on <code>snapshotOccurred</code> and initializes a  new instance of the index and subscribes that to consumer refreshes using <code>listenForDeltaUpdates()</code> (and detach the old index if necessary). <code>UniqueKeyIndex</code> implements this and can be used as a reference implementation.</p>"},{"location":"diving-deeper/#hollowprimarykeyindex","title":"HollowPrimaryKeyIndex","text":"<p>In the above example, the primary key is defined for <code>Movie</code> as its <code>id</code> field.  A primary key can also be defined over multiple and/or hierarchical fields.  Imagine that <code>Movie</code> additionally had a <code>country</code> field defined in its schema, and that across countries, <code>Movie</code> <code>id</code>s may be duplicated, but that there will never exist two <code>Movie</code> records with the same id and country: <pre><code>@HollowPrimaryKey(fields={\"id\", \"country.id\"})\npublic class Movie {\n    long id;\n    Country country;\n    ...\n}\n\n@HollowPrimaryKey(fields={\"id\"})\npublic class Country {\n    String id;\n    String name;\n}\n</code></pre></p> <p>A <code>HollowPrimaryKeyIndex</code> can be defined with a primary key consisting of both fields: <pre><code>HollowPrimaryKeyIndex idx =\n            new HollowPrimaryKeyIndex(readEngine, \"Movie\", \"id\", \"country.id.value\");\nidx.listenForDeltaUpdates(); // but does not listen for double-snapshot, see section on \"Keeping an index up-to-date\"\n</code></pre></p> <p>And to query for a <code>Movie</code> based on its id and country: <pre><code>int movieOrdinal = idx.getMatchingOrdinal(2, \"US\");\nif(movieOrdinal != -1) {\n    Movie movie = movieApi.getMovie(movieOrdinal);\n    System.out.println(\"Found Movie: \" + movie.getTitle().getValue());\n}\n</code></pre> Notice that <code>Movie</code>\u2019s country field in the above example is actually a <code>REFERENCE</code> field.  The defined key includes the id of the movie, and the value of the id String of the referenced country.  We denote this traversal using dot notation in the primary key definition.  The field definitions can be multiple references deep.</p> <p>The requirement for a primary key definition is that no duplicates should exist for the defined combination of fields.  If this rule is violated, an arbitrary match will be returned for queries when multiple matches exist.  </p> <p>Primary Key Violations</p> <p>Violations of the \"no duplicate\" primary key rule can be detected using the <code>getDuplicateKeys()</code> method on a <code>HollowPrimaryKeyIndex</code>, which returns a <code>Collection&lt;Object[]&gt;</code>.  If no duplicate keys exist, the returned Collection will be empty.  If they do, the returned values will indicate the keys for which duplicate records exist.</p> <p>If a <code>HollowPrimaryKeyIndex</code> will be retained for a long duration, they should be kept updated as deltas are applied to the underlying <code>HollowReadStateEngine</code>.  This is accomplished with a single call after instantiation to the <code>listenForDeltaUpdates()</code> method.</p> <p>Detaching Primary Key Indexes</p> <p>If <code>listenForDeltaUpdates()</code> is called on a primary key index, then it cannot be garbage collected.  If you intend to drop an index which is listening for updates, first call <code>detachFromDeltaUpdates()</code> to prevent a memory leak.</p> <p>Indexes which are listening for delta updates are updated after a dataset is updated.  In the brief interim time between when a dataset is updated and the index is updated, the index will point to the ghost records located at tombstoned ordinals.  This helps guarantee that all in-flight operations will observe correct data.</p>"},{"location":"diving-deeper/#hollowhashindex","title":"HollowHashIndex","text":"<p>It is sometimes desirable to index records by fields other than primary keys.  The <code>HollowHashIndex</code> allows for indexing records by fields or combinations of fields for which values may match multiple records, and records may match multiple values.</p> <p>In our <code>Movie</code>/<code>Actor</code> example, we may want to index movies by their starring actors: <pre><code>HollowHashIndex idx =\n            new HollowHashIndex(readEngine, \"Movie\", \"\", \"cast.element.actor.actorId\");\n</code></pre></p> <p>The <code>HollowHashIndex</code> expects in its constructor arguments a query start type, a select field, and a set of match fields.  The constructor arguments above indicate that queries will start with the <code>Movie</code> type, select the root of the query (indicated by the empty string), and match the id of any <code>Actor</code> record in the actors list.</p> <p>To query this index: <pre><code>HollowHashIndexResult result = idx.findMatches(102);\n\nif(result != null) {\n    System.out.println(\"Found matches: \" + result.numResults());\n\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        Movie movie = api.getMovie(matchedOrdinal);\n        System.out.println(\"Starred in: \" + movie.getTitle().getValue());\n        matchedOrdinal = iter.next();\n    }\n}\n</code></pre></p> <p>Alternatively, if the data model included the nationality of actors, and we needed to index actors by nationality and the titles of movies in which they starred: <pre><code>HollowHashIndex idx =\n            new HollowHashIndex(readEngine, \"Movie\", \"cast.element.actor\",\n                                            \"title.value\",\n                                            \"cast.element.actor.nationality.id.value\");\n</code></pre></p> <p>In this case, the query start type is still <code>Movie</code>, but we\u2019re selecting related <code>Actor</code> records.  Matches are selected based on the <code>Movie</code>\u2019s title, and the actor\u2019s nationality.  Using this index, one can query for Brazilian actors who starred in movies titled \u201cNarcos\u201d: <pre><code>HollowHashIndexResult result = idx.findMatches(\"Narcos\", \"BR\");\n\nif(result != null) {\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        Actor actor = api.getMovie(matchedOrdinal);\n        System.out.println(\"Matched actor: \" +\n                                      actor.getActorName().getValue());\n        matchedOrdinal = iter.next();\n    }\n}\n</code></pre></p> <p>The <code>HollowHashIndex</code> has the same facility and caveats for listening for delta updates as <code>HollowPrimaryKeyIndex</code> (see above section on \"Keeping an Index Up To Date\"), however unlike primary key index, hash index does a full re-index  even on delta updates whereas primary key index has the ability to refresh more efficiently on delta updates. This ability  for primary key index is under evaluation and it will be enabled by default at some point in the future.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>In the Quick Start guide, we got a reference implementation of Hollow up and running, with a mock data  model that can be easily modified to suit any use case.  After reading this section, you'll have an understanding of the  basic usage patterns for Hollow, and how each of the core pieces fit together.</p>"},{"location":"getting-started/#core-concepts","title":"Core Concepts","text":"<p>Hollow manages datasets which are built by a single producer, and disseminated to one or many consumers for  read-only access.  A dataset changes over time.  The timeline for a changing dataset can be broken down into discrete  data states, each of which is a complete snapshot of the data at a particular point in time.</p>"},{"location":"getting-started/#producing-a-data-snapshot","title":"Producing a Data Snapshot","text":"<p>Let's assume we have a POJO class <code>Movie</code>: <pre><code>@HollowPrimaryKey(fields=\"id\")\npublic class Movie {\n    long id;\n    String title;\n    int releaseYear;\n\n    public Movie(long id, String title, int releaseYear) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = releaseYear;\n    }\n}\n</code></pre></p> <p>And that many <code>Movie</code>s exist which comprise a dataset that needs to be disseminated: <pre><code>List&lt;Movie&gt; movies = Arrays.asList(\n        new Movie(1, \"The Matrix\", 1999),\n        new Movie(2, \"Beasts of No Nation\", 2015),\n        new Movie(3, \"Pulp Fiction\", 1994)\n);\n</code></pre></p> <p>We'll need a data producer to create a data state which will be transmitted to consumers: <pre><code>File localPublishDir = new File(\"/path/to/local/disk/publish/dir\");\n\nHollowFilesystemPublisher publisher = new HollowFilesystemPublisher(localPublishDir);\nHollowFilesystemAnnouncer announcer = new HollowFilesystemAnnouncer(localPublishDir);\n\nHollowProducer producer = HollowProducer\n        .withPublisher(publisher)\n        .withAnnouncer(announcer)\n        .build();\n\nproducer.runCycle(state -&gt; {\n    for(Movie movie : movies)\n        state.add(movie);\n});\n</code></pre></p> <p>This producer runs a single cycle and produces a data state.  Once this runs, you should have a snapshot blob file  on your local disk.  </p> <p>Publishing Blobs</p> <p>Note that the example code above is writing data to local disk.  This is a great way to start testing.  In a  production scenario, data can be written to a remote file store such as Amazon S3 for retrieval by consumers.  See  the reference implementation and the  quick start guide for a scalable example using AWS.</p>"},{"location":"getting-started/#consumer-api-generation","title":"Consumer API Generation","text":"<p>Once the data has been populated into a producer, that producer's state engine is aware of the data model, and can be  used to automatically produce a client API.  We can also initialize the data model from a brand new state engine using  our POJOs:</p> <pre><code>HollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nmapper.initializeTypeState(Movie.class);\n\nHollowAPIGenerator generator =\n       new HollowAPIGenerator.Builder().withAPIClassname(\"MovieAPI\")\n                                       .withPackageName(\"how.hollow.example\")\n                                       .withDataModel(writeEngine)\n                                       .build();\n\ngenerator.generateFiles(\"/path/to/java/api/files\");\n</code></pre> <p>After this code executes, a set of Java files will be written to the location <code>/path/to/java/api/files</code>.  These java  files will be a generated API based on the data model defined by the schemas in our state engine, and will provide  convenient methods to access that data.</p> <p>Initializing multiple types</p> <p>If we have multiple top-level types, we should call <code>initializeTypeState()</code> multiple times, once for each class.</p>"},{"location":"getting-started/#consuming-a-data-snapshot","title":"Consuming a Data Snapshot","text":"<p>A data consumer can load a snapshot created by the producer into memory: <pre><code>File localPublishDir = new File(\"/path/to/local/disk/publish/dir\");\n\nHollowFilesystemBlobRetriever blobRetriever =\n                                new HollowFilesystemBlobRetriever(localPublishDir);\n\nHollowFilesystemAnnouncementWatcher announcementWatcher =\n                                new HollowFilesystemAnnouncementWatcher(localPublishDir);\n\nHollowConsumer consumer = HollowConsumer.withBlobRetriever(blobRetriever)\n                                        .withAnnouncementWatcher(announcementWatcher)\n                                        .withGeneratedAPIClass(MovieAPI.class)\n                                        .build();\n\nconsumer.triggerRefresh();\n</code></pre></p> <p>The <code>HollowConsumer</code> will retrieve data using the provided <code>BlobRetrievier</code>, and will load the latest data state  currently announced by the <code>AnnouncementWatcher</code>.</p> <p>Once this dataset is loaded into memory, we can access the data for any records using our generated API.  Below, we're  iterating over all records: <pre><code>MovieAPI movieApi = (MovieAPI)consumer.getAPI();\n\nfor(MovieHollow movie : movieApi.getAllMovieHollow()) {\n    System.out.println(movie.getId() + \", \" +\n                       movie.getTitle().getValue() + \", \" +\n                       movie.getReleaseYear());\n}\n</code></pre></p> <p>The output of the above code will be: <pre><code>1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n3, Pulp Fiction, 1994\n</code></pre></p> <p>Integrating with Infrastructure</p> <p>In order to integrate with your infrastructure, you only need to provide Hollow with four implementations of simple  interfaces:</p> <ul> <li>The <code>HollowProducer</code> needs a <code>Publisher</code> and <code>Announcer</code></li> <li>The <code>HollowConsumer</code> needs a <code>BlobRetriever</code> and <code>AnnouncementWatcher</code></li> </ul> <p>Your <code>BlobRetriever</code> and <code>AnnouncementWatcher</code> implementations should be mirror your <code>Publisher</code> and <code>Announcer</code>  interfaces.   Here, we're publishing and retrieving from local disk.  In production, we'll be publishing to and  retrieving from a remote file store.  We'll discuss in more detail how to integrate with your specific  infrastructure in Infrastructure Integration.</p>"},{"location":"getting-started/#producing-a-delta","title":"Producing a Delta","text":"<p>Some time has passed and the dataset has evolved.  It now contains these records: <pre><code>List&lt;Movie&gt; movies = Arrays.asList(\n        new Movie(1, \"The Matrix\", 1999),\n        new Movie(2, \"Beasts of No Nation\", 2015),\n        new Movie(4, \"Goodfellas\", 1990),\n        new Movie(5, \"Inception\", 2010)\n);\n</code></pre></p> <p>The producer, needs to communicate this updated dataset to consumers.  We're going to create a brand new state, and the  entirety of the data for the new state must be added to the state engine in a new cycle.   When the cycle runs, a new  data state will be published, and the new data state's (automatically generated) version identifier will be  announced.</p> <p>Using the same <code>HollowProducer</code> in memory, we can use the following code:</p> <pre><code>producer.runCycle(state -&gt; {\n    for(Movie movie : movies)\n        state.add(movie);\n});\n</code></pre> <p>Let's take a closer look at what the above code does.  The same <code>HollowProducer</code> which was used to produce the  snapshot blob is used -- it already knows everything about the prior state and can be transitioned to the next state. When creating a new state, all of the movies currently in our dataset are re-added again.  It's not necessary to  figure out which records were added, removed, or modified -- that's Hollow's job.</p> <p>Each time we call <code>runCycle</code> we will be producing a data state.  For each state after the first, the <code>HollowProducer</code>  will publish three artifacts: a snapshot, a delta, and a reverse delta.  Encoded into the delta is a set of  instructions to update a consumer\u2019s data store from the previous state to the current state.  Inversely, encoded into  each reverse delta is a set of instructions to update a consumer in reverse -- from the current state to the previous  state.  Consumers may use the reverse delta later if we need to pin.</p> <p>When consumers initialize, they will use the most recent snapshot to initialize their data store.  After  initialization, consumers will keep up to date using deltas.</p> <p>Producer Cycles</p> <p>We call what the producer does to create a data state a cycle.  During each cycle, you\u2019ll want to add  every record from your source of truth.  Hollow will handle the details of publishing a delta for all of your  established consumer instances, and a snapshot to initialize any consumer instances which start up before your next  cycle.</p>"},{"location":"getting-started/#consuming-a-delta","title":"Consuming a Delta","text":"<p>No manual intervention is necessary to consume the delta you produced.  The <code>HollowConsumer</code> will automatically stay  up-to-date.  </p> <p>Announcements keep consumers updated</p> <p>When the producer runs a cycle, it announces the latest version.  The <code>AnnouncementWatcher</code> implementation  provided to the <code>HollowConsumer</code> will listen for changes to the announced version -- and when updates occur notify  the <code>HollowConsumer</code> by calling <code>triggerAsyncRefresh()</code>.  See the source of the  <code>HollowFilesystemAnnouncementWatcher</code>, or the  two  separate examples  in the reference implementation.</p> <p>After this delta has been applied, the consumer is at the new state.  If the generated API is used to iterate over the  movies again as shown in the prior consumer example, the new output will be:</p> <pre><code>1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n4, Goodfellas, 1990\n5, Inception, 2010\n</code></pre> <p>Thread Safety</p> <p>It is safe to use Hollow to retrieve data while a delta transition is in progress.</p> <p>Adjacent States</p> <p>We refer to states which are directly connected via single delta transitions as adjacent states, and a continuous  set of adjacent states as a delta chain</p>"},{"location":"getting-started/#incremental-production","title":"Incremental Production","text":"<p>If it is known what changes are applied to a dataset then incremental production may be utilized.  This can be more efficient than providing the whole dataset on each cycle.  An incremental producer is built in a similar manner to a producer:</p> <pre><code>File localPublishDir = new File(\"/path/to/local/disk/publish/dir\");\n\nHollowFilesystemPublisher publisher = new HollowFilesystemPublisher(localPublishDir);\nHollowFilesystemAnnouncer announcer = new HollowFilesystemAnnouncer(localPublishDir);\n\nHollowProducer.Incremental incProducer = HollowProducer\n        .withPublisher(publisher)\n        .withAnnouncer(announcer)\n        .buildIncremental();\n\nincProducer.runIncrementalCycle(istate -&gt; {\n    for(Movie movie : modifiedMovies)\n        state.addOrModify(movie);\n    for(Movie movie : deletedMovies)\n        state.delete(movie);\n});\n</code></pre> <p>This incremental producer runs a cycle for the changes to set of movies (those which are new, have changed, or have been  removed).  Other than that an incremental producer behaves the same as a producer and a consumer will not know the difference.</p> <p>Adjacent States</p> <p>Any record added, modified or removed must have a primary key, since this determines the record's identity</p>"},{"location":"getting-started/#indexing-data-for-retrieval","title":"Indexing Data for Retrieval","text":"<p>In prior examples the generated Hollow API was used by the data consumer to iterate over all <code>Movie</code> records in the  dataset.  Most often, however, it isn\u2019t desirable to iterate over the entire dataset \u2014 instead, specific records will be  accessed based on some known key.  The <code>Movie</code>\u2019s id is a known key (since it is annotated with <code>@HollowPrimaryKey</code>).</p> <p>After a <code>HollowConsumer</code> has been initialized, any type can be indexed.  For example, we can index <code>Movie</code> records by  <code>id</code>: <pre><code>HollowConsumer consumer = ...;\n\nconsumer.triggerRefresh();\n\nUniqueKeyIndex&lt;Movie, Integer&gt; idx = Movie.uniqueIndex(consumer);\nconsumer.addRefreshListener(idx);  // tell index to listen for consumer updates\n</code></pre></p> <p>This index can be held in memory and then used in conjunction with the generated Hollow API to retrieve Movie records by  id: <pre><code>Movie movie = idx.findMatch(2);\nif(movie != null)\n    System.out.println(\"Found Movie: \" + movie.getTitle().getValue());\n</code></pre></p> <p>Which outputs: <pre><code>Found Movie: Beasts of No Nation\n</code></pre></p> <p>In our generated API, each type annotated with <code>@HollowPrimaryKey</code> has a static method to obtain a <code>UniqueIndex</code>.  For primary keys with multiple fields a bean class is also generated to hold key values.</p> <p>A <code>UniqueIndex</code> may be also created explicitly to index by any field, or multiple fields.</p> <p>Reuse Indexes</p> <p>Retrieval from an index is extremely cheap, and indexing is (relatively) expensive.  You should create your indexes  when the <code>HollowConsumer</code> is initialized and share them thereafter.  Indexes will automatically stay up-to-date with  the <code>HollowConsumer</code>.</p> <p>Thread Safety</p> <p>Retrievals from Hollow indexes are thread-safe.  They are safe to use across multiple threads, and it is safe to  query while a transition is in progress.</p> <p>We've just begun to scratch the surface of what indexes can do.  See Indexing/Querying for an  in-depth exploration of this topic.</p>"},{"location":"getting-started/#hierarchical-data-models","title":"Hierarchical Data Models","text":"<p>Our data models can be much richer than in the prior example.  Assume an updated <code>Movie</code> class: <pre><code>public class Movie {\n    long id;\n    String title;\n    int releaseYear;\n    List&lt;Actor&gt; actors;\n\n    public Movie(long id, String title, int year, List&lt;Actor&gt; actors) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = year;\n        this.actors = actors;\n    }\n}\n</code></pre></p> <p>Which references <code>Actor</code> records: <pre><code>@HollowPrimaryKey(field=\"actorId\")\npublic class Actor {\n    long actorId;\n    String actorName;\n\n    public Actor(long actorId, String actorName) {\n        this.actorId = actorId;\n        this.actorName = actorName;\n    }\n}\n</code></pre></p> <p>Some records are added to a <code>HollowProducer</code>: <pre><code>List&lt;Movie&gt; movies = Arrays.asList(\n        new Movie(1, \"The Matrix\", 1999, Arrays.asList(\n                new Actor(101, \"Keanu Reeves\"),\n                new Actor(102, \"Laurence Fishburne\"),\n                new Actor(103, \"Carrie-Ann Moss\"),\n                new Actor(104, \"Hugo Weaving\")\n        )),\n        new Movie(6, \"Event Horizon\", 1997, Arrays.asList(\n                new Actor(102, \"Laurence Fishburne\"),\n                new Actor(105, \"Sam Neill\")\n        ))\n);\n\nproducer.runCycle(state -&gt; {\n    for(Movie movie : movies)\n        state.add(movie);\n});\n</code></pre></p> <p>When we add these movies to the dataset, Hollow will traverse everything referenced by the provided records and add them  to the state as well.  Consequently, both a type <code>Movie</code> and a type <code>Actor</code> will exist in the data model after the above  code runs.  </p> <p>Deduplication</p> <p>Laurence Fishburne starred in both of these films.  Rather than creating two <code>Actor</code> records for Mr. Fishburne,  a single record will be created and assigned to both of our <code>Movie</code> records.  This deduplication happens  automatically by virtue of having the exact same data contained in both Actor inputs.</p> <p>Consumers of this dataset may want to also create an index for <code>Actor</code> records.  For example: <pre><code>UniqueKeyIndex&lt;Actor, Integer&gt; idx = Actor.uniqueIndex(consumer);\n\nActor actor = actorIdx.findMatch(102);\nif(actor != null)\n    System.out.println(\"Found Actor: \" + actor.getActorName().getValue());\n</code></pre></p> <p>Outputs: <pre><code>Found Actor: Laurence Fishburne\n</code></pre></p>"},{"location":"getting-started/#restoring-at-startup","title":"Restoring at Startup","text":"<p>From time to time, we need to redeploy our producer.  When we first create a <code>HollowProducer</code> and run a cycle it will  not be able to produce a delta, because it does not know anything about the prior data state.  If no action is taken,  a new state with only a snapshot will be produced and announced, and clients will load that data state with an operation  called a double snapshot, which has potentially undesirable performance  characteristics.  </p> <p>We can remedy this situation by restoring our newly created producer with the last announced data state.  For example:</p> <pre><code>Publisher publisher = ...\nAnnouncer announcer = ...\nBlobRetriever blobRetriever = ...\nAnnouncementWatcher announcementWatcher = ...\n\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .build();\n\nproducer.initializeDataModel(Movie.class);\n\nlong latestAnnouncedVersion = announcementWatcher.getLatestVersion();\nproducer.restore(latestAnnouncedVersion, blobRetriever);\n\nproducer.runCycle(state -&gt; {\n   ...\n});\n</code></pre> <p>In the above code, we first initialize the data model by providing the set of classes we will add during the cycle. After that, we restore by providing our <code>BlobRetriever</code> implementation, along with the version which should be  restored.  The <code>HollowProducer</code> will use the <code>BlobRetriever</code> to load the desired state, then use it to restore itself. In this way, a delta can be produced at startup, and consumers will not have to load a snapshot to get up-to-date.</p> <p>Initializing the data model</p> <p>Before restoring, we must always initialize our data model.  When a data model changes between deployments,  Hollow will automatically merge records of types which have changed.  In order to do this correctly, Hollow needs to  know about the current data model before the restore operation begins.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>adjacent state</p> <p>If state <code>A</code> is connected via a single delta to state <code>B</code>, then <code>A</code> and <code>B</code> are adjacent to each other.</p> <p>announce</p> <p>After the blobs for a state have been published to a blob store by a producer, the state must be announced to consumers.  The announcement signals to consumers that they should transition to the announced state.</p> <p>blob</p> <p>A blob is a file used by consumers to update their dataset.  A blob will be either a snapshot, delta, or reverse delta</p> <p>blob store</p> <p>A blob store is a file store to which blobs can be published by a producer and retrieved by a consumer. </p> <p>broken delta chain</p> <p>When a blob namespace contains a state which is not adjacent to any prior states, the delta chain is said to be broken.  In this scenario, consumers may need to load a double snapshot.</p> <p>consumer</p> <p>One of many machines on which a dataset is made accessible.  Consumers are updated in lock-step based on the actions of the producer.</p> <p>cycle</p> <p>A producer runs in an infinite loop.  Each exection of the loop is called a cycle.  Each cycle produces a single data state.</p> <p>data model</p> <p>A data model defines the structure of a dataset.  It is specified with a set of schemas.</p> <p>data state</p> <p>A dataset changes over time.  The timeline for a changing dataset can be broken down into discrete data states, each of which is a complete snapshot of the data at a particular point in time.</p> <p>deduplication</p> <p>Two records which have identical data in Hollow will be consolidated into a single record.  Any references to duplicate records will be mapped to the canonical one when a dataset is represented with Hollow.</p> <p>delta</p> <p>A set of encoded instructions to transition from one data state to an adjacent state.  Deltas are encoded as a set of ordinals to remove and a set of ordinals to add, along with the accompanying data to add.  'Delta' may refer specifically to a transition between an earlier state and a later state, contrasted with 'reverse delta', which specifically refers to a transition between a later state and an earlier state.</p> <p>delta chain</p> <p>A series of states which are all connected via contiguous deltas.</p> <p>diff</p> <p>A comprehensive accounting for the differences between two data states.</p> <p>double snapshot</p> <p>When a consumer already has an initialized state and an announcement signals to move to a new state for which a path of deltas is not available, the consumer may transition to that state via a snapshot.  In this scenario two full copies of the dataset must be loaded in memory.</p> <p>field</p> <p>A single value encoded inside of a Hollow record.</p> <p>hash key</p> <p>A user-defined specification of one or more fields used to hash elements into a set or entries into a map.</p> <p>ingestion</p> <p>Gathering data from a source of truth and importing it into Hollow.</p> <p>inline</p> <p>A field for which the value is encoded directly into a record, as opposed to referenced via another record.</p> <p>namespace (blobs)</p> <p>An addressable, logical separation of both published artifacts in a blob store and announcement location.  Used to allow multiple publishers to communicate on separate channels to specific groups of consumers.</p> <p>namespace (references)</p> <p>The deliberate creation of a type to hold a specific referenced field's data in order to reduce the cardinality of the referenced records.</p> <p>object longevity</p> <p>A technique used to ensure that stale references to Hollow Objects always return the same data they did initially upon creation.  Configured via the <code>HollowObjectMemoryConfig</code>.</p> <p>ordinal</p> <p>An integer value uniquely identifying a record within a type.  Because records are represented with a fixed-length number of bits, the only necessary information to locate a record in memory is the record's type and ordinal.  Ordinals are automatically assigned by Hollow, and are recycled as records are removed and added.  Consequently, they lie in the range of 0-n, where n is generally not much larger than the total number of records for the type.</p> <p>patch (states)</p> <p>Creating a series of two deltas between states in a delta chain.</p> <p>pinning</p> <p>Overriding the state version announcement from the producer, to force clients to go back to or stay at an older state.</p> <p>primary key</p> <p>A user-defined specification of one or more fields used to uniquely identify a record within a type.</p> <p>producer</p> <p>A single machine that retrieves all data from a source of truth and produces a delta chain.</p> <p>publish</p> <p>Writing blobs to a blob store.</p> <p>read state engine</p> <p>A <code>HollowReadStateEngine</code>, the root handle to a Hollow dataset as a consumer.</p> <p>record</p> <p>A strongly-typed collection of fields or references, the structure of which is specified by a schema.</p> <p>reference</p> <p>A field type which indicates a pointer to another field.  Can also refer to the technique of pulling out a specific field into a record type of its own to deliberately allow Hollow to deduplicate the values.</p> <p>restore</p> <p>Initializing a <code>HollowWriteStateEngine</code> with data from a previously produced state so that a delta may be created during a producer's first cycle.</p> <p>reverse delta</p> <p>A delta from a later state to an earlier state.  Generally used during pinning scenarios.</p> <p>schema</p> <p>Metadata about a Hollow type which defines the structure of the records.</p> <p>snapshot</p> <p>A blob type which contains a serialization of all of the records in a type.  Consumed during initialization, and possibly in a broken delta chain scenario.</p> <p>state</p> <p>See data state.</p> <p>state version</p> <p>A unique identifier for a state.  Should by monotonically increasing as time passes.</p> <p>state engine</p> <p>Both the producer and consumers handle datasets with a state engine.  A state engine can be transitioned between data states.  A producer uses a write state engine and a consumer uses a read state engine</p> <p>type</p> <p>A collection of records all conforming to a specific schema.</p> <p>write state engine</p> <p>A <code>HollowWriteStateEngine</code>, the root handle to a Hollow dataset as a consumer.</p>"},{"location":"indexing-querying/","title":"Indexing/Querying","text":"<p>Hollow supports indexing of your dataset for quick retrieval. This guide demonstrates on usages of different indexes available and how they could be used to query for faster retrieval. However, indexes by default do not recalculate their state when the consumer refreshes to a new data state. As such, results for queries may become stale or corrupt if the consumer is refreshed. Refer to Keeping an index up-to-date for more details.</p>"},{"location":"indexing-querying/#a-data-model","title":"A Data Model","text":"<p>For the purposes of these examples, let's imagine we have a data model defined by the following Objects:</p> <pre><code>@HollowPrimaryKey(fields=\"id\")\npublic class Movie {\n    int id;\n    String title;\n\n    @HollowHashKey(fields=\"actor.actorId\")\n    Set&lt;ActorRole&gt; cast;\n\n    CountryCode releaseCountry;\n}\n\npublic class ActorRole {\n    Actor actor;\n    int movieId;\n    String characterName;\n}\n\n@HollowPrimaryKey(fields=\"actorId\")\npublic class Actor {\n    int actorId;\n    String name;\n}\n\nenum CountryCode {\n    US, CA, ME\n}\n</code></pre>"},{"location":"indexing-querying/#primary-key-indexes","title":"Primary Key Indexes","text":"<p>When we generate a client API, each type in our data model gets a custom index class called <code>&lt;typename&gt;PrimaryKeyIndex</code>.  We can use these classes to look up records based on primary key values.</p>"},{"location":"indexing-querying/#default-primary-keys","title":"Default Primary Keys","text":"<p>Once we have loaded a dataset into a <code>HollowConsumer</code>, we can use the <code>Movie</code> index to retrieve data by the default primary key <code>id</code>:</p> <pre><code>HollowConsumer consumer = ...;\n\nMoviePrimaryKeyIndex idx = new MoviePrimaryKeyIndex(consumer, true); // param isListenToDataRefresh set to true to enable listening to data refresh\n\nint knownMovieId = ...;\n\nMovie movie = idx.findMatch(knownMovieId);\n</code></pre> <p>Note that keeping the primary key index up-to-date with the <code>HollowConsumer</code> data state changes is optional.</p> <p>Share Indexes</p> <p>Queries to indexes are thread-safe.  We should create each of the indexes we need only once, and share them everywhere they are needed.</p>"},{"location":"indexing-querying/#consumer-specified-primary-keys","title":"Consumer-specified Primary Keys","text":"<p>In the prior example, our primary key index was using the default primary key defined in the data model.  A primary key index is not restricted to just default primary keys.  For example, we could also index movies by their title:</p> <pre><code>MoviePrimaryKeyIndex idx = new MoviePrimaryKeyIndex(consumer, \"title\", true); // param isListenToDataRefresh set to true to enable listening to data refresh\n\nString knownMovieTitle = ...;\n\nMovie movie = idx.findMatch(knownMovieTitle);\n</code></pre> <p>Primary Keys</p> <p>A primary key index should be used when there is a one-to-one mapping between records and key values.  A primary key can only return one record per key, and if multiple records exist for a given key, then an arbitrary match will be returned.</p>"},{"location":"indexing-querying/#compound-primary-keys","title":"Compound Primary Keys","text":"<p>A primary key index may also be specified over multiple fields.  For example, we can define a primary key index for the <code>ActorRole</code> type above:</p> <pre><code>ActorRolePrimaryKeyIndex idx = new ActorRolePrimaryKeyIndex(consumer, \"actor.id\", \"movieId\");\n\nint knownActorId = ...;\nint knownMovieId = ...;\n\nActorRole actorRoleInMovie = idx.findMatch(knownActorId, knownMovieId);\n</code></pre> <p>In the above example, we are looking for the actor role which matches both the actor ID and the movie ID.  Note that the actor id was specified with dot-notation as <code>actor.id</code>.  This is a field path, and indicates that the actual value we're indexing belongs to a referenced record.  Note that for a primary key index, we can only traverse through referenced <code>Object</code> records, not <code>List</code>, <code>Set</code>, or <code>Map</code> records.  We'll cover more about field paths a bit further down.</p>"},{"location":"indexing-querying/#hash-indexes","title":"Hash Indexes","text":"<p>If we want to find records based on keys for which there is not a one-to-one mapping between records and key values, we want a hash index.  With our generated client API, we have a single class <code>&lt;API classname&gt;HashIndex</code>.  We can use instances of this class to specify hash indexes.  A hash index must specify each of a query type, a select field, and one or more match fields.  If we want to select the same type we are using to search, we should specify our select field as and empty String <code>\"\"</code>.</p> <p>For example, if we want to match <code>Movie</code> records which had characters with some name, we can use the following:</p> <pre><code>MovieAPIHashIndex idx = new MovieAPIHashIndex(\"Movie\", \"\", \"cast.element.characterName.value\");\n\nString knownCharacterName = ...;\n\nfor(Movie movie : idx.findMovieMatches(knownCharacterName)) {\n    System.out.println(\"The movie \" + movie.getTitle().getValue() +\n                       \" has a character named \" + knownCharacterName);\n}\n</code></pre> <p>Above, we are selecting the same type from which our query is derived.  However, if we wanted to find <code>Actor</code> records which starred in <code>Movie</code> records that have a specific title, we need to formulate our query at the <code>Movie</code> level, but we are selecting a different node:</p> <pre><code>MovieAPIHashIndex idx = new MovieAPIHashIndex(\"Movie\", \"cast.element.actor\", \"title.value\");\n\nString knownMovieTitle = ...;\n\nfor(Actor actor : idx.findActorMatches(knownMovieTitle)) {\n    System.out.println(\"The actor \" + actor.getName().getValue() +\n                       \" starred in \" + knownMovieTitle);\n}\n</code></pre> <p>We can also match at multiple places in a type hierarchy.  For example, if we want to find the <code>ActorRole</code> by actor id and movie title, we can use the following:</p> <p><pre><code>MovieAPIHashIndex idx = new MovieAPIHashIndex(\"Movie\", \"cast.element\",\n                                              \"cast.element.actor.actorId\", \"title.value\");\n\nString knownMovieTitle = ...;\nint knownActorId = ...;\n\nfor(ActorRole role : idx.findActorRoleMatches(knownActorId, knownMovieTitle)) {\n    System.out.println(\"The actor \" + role.getActor().getName().getValue() +\n                       \" starred in \" + knownMovieTitle +\n                       \" as \" + role.getCharacterName().getValue());\n}\n</code></pre> Similarly, if we want to include an enum type like releaseCountry in the fields, then its field path in the index construction can be specified as <code>releaseCountry._name</code>. Note, in field paths, an enum type is treated slightly differently from a String reference type which is expanded using <code>.value</code>.</p>"},{"location":"indexing-querying/#field-paths","title":"Field Paths","text":"<p>A field path indicates how to traverse through a type hierarchy. It contains multiple parts delimited by <code>.</code>, and we need one part per type through which we're traversing. Each part corresponding to an <code>OBJECT</code> type should be equal to the name of a field in that type.</p>"},{"location":"indexing-querying/#primary-and-hash-keys","title":"Primary and hash keys","text":"<p>Primary key and hash key field paths may only span through <code>OBJECT</code> types.  These field paths will be automatically expanded if they end in a <code>REFERENCE</code> field which points to a type that has only a single field, or a type which has a primary key with only a single field defined.  If auto-expansion is not desired, the field path should terminate with a <code>!</code> character.  For example, in our data model example above, the following field paths for the type <code>Movie</code> are equivalent: <code>title</code>, <code>title.value</code>.  If we actually want the field path to terminate at the <code>REFERENCE</code> field <code>title</code>, we can specify the field path as <code>title!</code>.</p>"},{"location":"indexing-querying/#hash-indexes_1","title":"Hash indexes","text":"<p>Hash index field paths may span through any type.  Each part corresponding to a <code>LIST</code> or <code>SET</code> type should be specified as <code>element</code>. Similarly, each part corresponding to a <code>MAP</code> type should be specified as either <code>key</code> or <code>value</code>.  Hash index field paths are never auto-expanded. When providing an enum type in the field path, use enum field name followed by <code>._name</code>. For example, <pre><code>HashIndex hashIndex = HashIndex\n.from(consumer, Movie.class)\n.usingPath(\"releaseCountry._name\", String.class);\n</code></pre></p>"},{"location":"indexing-querying/#hash-keys","title":"Hash Keys","text":"<p>Notice that in the POJOs of our data model defined at the beginning of this topic, we annotated the <code>Set&lt;ActorRole&gt;</code> in the <code>Movie</code> type with <code>@HollowHashKey(fields=\"actor.actorId\")</code>.  This means that for each of these sets, the data will be hashed by the actor ID in the contained record.  In our generated API, we can easily find any record by actor ID using the <code>findElement()</code> method.  For example:</p> <pre><code>Movie movie = ...;\nint knownActorId = ...;\n\nActorRole role = movie.getCast().findElement(knownActorId);\n</code></pre> <p>In this way, each of our set records can be indexed by any field, or combination of fields, for O(1) retrieval of contained records.  The rules for defining a hash key are similar to the rules for defining a primary key:</p> <ul> <li>Compound hash keys may be defined by specifying multiple fields.</li> <li>Field paths may only span through <code>OBJECT</code> types.</li> <li>Field paths will be auto-expanded if they terminate in a <code>REFERENCE</code> field.</li> <li>Should be used when there is a one-to-one mapping between records and keys per set.  If duplicates exist, an arbitrary valid match will be returned.</li> </ul> <p>If defined on a set type, hash key field paths should be defined starting from the element type.</p> <p>Hash keys may also be defined on map record types.  When defined on a map record, the field paths should be defined starting from the key type.  The methods <code>findKey()</code>, <code>findValue()</code>, and <code>findElement()</code> are available on map types in the generated API for consumers to look up records by hash key values.  </p> <p>If using the <code>HollowObjectMapper</code>, unspecified hash keys will be automatically selected if an element or key type contain a single non-reference field.  Addionally, if a <code>Set</code> or <code>Map</code> references <code>Object</code> elements with a defined primary key, then the hash key will default to the primary key of the element type.  Alternatively, hash keys can be explicitly defined using the <code>@HollowHashKey</code> annotation in POJOs for <code>Set</code> schemas by specifying one or more fields from the element type, or for <code>Map</code> schemas by specifying one or more fields from the key type.  See our data model example at the beginning of this section for an example.</p>"},{"location":"indexing-querying/#field-match-scan-queries","title":"Field Match Scan Queries","text":"<p>Each of the examples above pre-index your dataset to achieve O(1) lookup times.  These are very efficient, but require pre-knowledge of what you're searching for. Given that all of hollow datasets exist in memory, for some use cases it is reasonable to scan through the entire dataset looking for matches.</p> <p>The <code>HollowFieldMatchQuery</code> can be used to accommodate these use cases.  The Hollow Explorer UI, for example, uses this mechanism to provide a powerful \"search-for-anything\" capability with reasonable response times for low-volume query traffic.</p>"},{"location":"indexing-querying/#diving-deeper","title":"Diving Deeper","text":"<p>Lower-level interfaces are available to index data in the absence of a generated API.  See Diving Deeper: Indexing Data for Retrieval for a detailed look.</p>"},{"location":"infrastructure/","title":"Infrastructure Integration","text":"<p>In order to wield Hollow effectively for your organization, you need only implement four interfaces to integrate with your infrastructure:</p> <ul> <li>A <code>Publisher</code> for the <code>HollowProducer</code></li> <li>An <code>Announcer</code> for the <code>HollowProducer</code></li> <li>A <code>BlobRetriever</code> for the <code>HollowConsumer</code></li> <li>An <code>AnnouncementWatcher</code> for the <code>HollowConsumer</code></li> </ul> <p>Once you've implemented these four interfaces, Hollow can be used in many different contexts in your organization.  You'll never again have to write code to ship json or csv data from one machine to another -- and better yet, you'll gain visibility and insights into previously opaque and hard-to-debug datasets.</p> <p>Think local first</p> <p>The following sections describe how to plug Hollow into your infrastructure.  Now is the time to think about how you will debug your data later.  Consider making your implementations of these interfaces easily allow for (securely!) retrieving data from any environment, including production, right down to your local development box.</p> <p>If you take this step, you'll be giving yourself immense power to glean insight into your data and debug production issues.  Imagine it's 10am, and you suspect some issue surrounding some particular data was present at 4am this morning.  You can open up Eclipse or IntelliJ and write a main method which -- with a few lines of code -- pulls down the data exactly as it existed on your production instances at that time.  You can write code against it to explore specific scenarios or feed it into the explorer to confirm or deny your suspicion in seconds.</p>"},{"location":"infrastructure/#storing-the-blobs","title":"Storing the Blobs","text":"<p>Blobs are published to a file store which is accessible by consumers.  From this blob store, consumers must be able to query for and retrieve blobs in the following ways:</p> <ul> <li>Snapshots: Must be queryable based on the state identifier.  If a blob store is queried for a snapshot with an identifier which does not exist, the snapshot with the greatest identifier prior to the queried identifier should be retrieved.</li> <li>Deltas: Must be queryable based on the state identifier to which a delta should be applied (e.g. the delta's from state identifier).</li> <li>Reverse Deltas: Must be queryable based on the state identifier to which a reverse delta should be applied (e.g. the reverse delta's from state identifier).</li> </ul> <p>The <code>Publisher</code> and <code>BlobRetriever</code> are opposite sides of your blob store (writer and reader, respectively).</p> <p>Your <code>Publisher</code> implementation must only define a single method:</p> <pre><code>    public void publish(HollowProducer.Blob blob);\n</code></pre> <p>The <code>Blob</code> passed to your <code>Publisher</code> should be published somewhere for retrieval by consumers.  The blob's data is retrieved for publish by calling either <code>newInputStream()</code> or <code>getFile()</code>.  The blob will be one of either a snapshot, delta, or reverse delta -- the type can be determined by calling <code>getType()</code>.  The blob should be indexed for later retrieval as indicated above -- snapshots by the result of <code>getToVersion()</code>, and deltas/reversedeltas by the result of <code>getFromVersion()</code>.  Note that you will need to be able to distinguish between a snapshot, delta, and reversedelta with the same version number.  </p> <p>Choosing a blob store</p> <p>You can publish blobs anywhere -- S3, an FTP server, an NFS, etc -- so long as that selected blob store can scale to the necessary volume of concurrent consumer requests.  </p> <p>Note that if your announcement mechanism is instantaneous all consumers will attempt to retrieve the blob files simultaneously.</p> <p>Blobs must be overwriteable</p> <p>Your <code>Publisher</code> implementation must allow blobs to be overwritten.  If an attempt is made to publish a blob with to be indexed by a state identifier for which a corresponding artifact already exists, it must overwrite the existing artifact previously published.  This happens routinely -- for example if a data state fails after publishing for any reason (e.g. validation fails), then the producer will automatically roll back the state and a delta will be re-published with the same from version.</p> <p>The <code>BlobRetriever</code> is the other side of the blob store equation.  Your implementation must define three methods:</p> <pre><code>    public HollowConsumer.Blob retrieveSnapshotBlob(long desiredVersion);\n\n    public HollowConsumer.Blob retrieveDeltaBlob(long currentVersion);\n\n    public HollowConsumer.Blob retrieveReverseDeltaBlob(long currentVersion);\n</code></pre> <p>The <code>Blob</code> you return will be a custom implementation for your blob store which extends <code>HollowConsumer.Blob</code> and implements the <code>getInputStream()</code> method.  </p> <p>Your <code>retrieveSnapshotBlob(long desiredVersion)</code> implementation should return the blob which exactly matches the specified <code>desiredVersion</code> if it exists.  If no such version exists then the greatest available version which is less than the specified <code>desiredVersion</code> should be returned.  If no such match exists, return null.</p> <p>Your <code>retrieveDeltaBlob(long currentVersion)</code> and <code>retrieveReverseDeltaBlob(long currentVersion)</code> implementations should each return the blob which exactly matches the specified <code>currentVersion</code>.  If no such match exists, return null.</p> <p>Scanning for snapshots</p> <p>If an exact match for the requested snapshot doesn't exist, you'll need to scan the available versions for the closest match prior to the requested.  For this reason, if you have a large number of consumers, it makes sense to index your available snapshot versions so this operation is fast.</p>"},{"location":"infrastructure/#announcing-the-state","title":"Announcing the State","text":"<p>Once the necessary transitions to bring clients up to date have been written to the blob store, the availability of the state must be announced to clients.  This simply means that a centralized location must be maintained and updated by the producer which indicates the version of the currently available state.  </p> <p>When this announced state is updated, usually it is desirable to have consumers realize this update as quickly as possible.  This can be accomplished either via a push notification to all consumers, or via frequent polling by consumers.</p> <p>The <code>Announcer</code> and <code>AnnouncementWatcher</code> are opposite sides of your announcement mechanism (writer and reader, respectively).</p> <p>Your <code>Announcer</code> implementation must only implement a single method:</p> <pre><code>    public void announce(long stateVersion);\n</code></pre> <p>The <code>stateVersion</code> passed to your <code>Announcer</code> should be immediately communicated to your consumers.  You can use either a 'push' mechanism or a frequent 'polling' mechanism to minimize the time between when a producer announces a version, and all consumers receive that announcement.</p> <p>Your <code>AnnouncementWatcher</code> implementation must implement two methods:</p> <pre><code>    public long getLatestVersion();\n\n    public void subscribeToUpdates(HollowConsumer consumer);\n</code></pre> <p>When your <code>AnnouncementWatcher</code> is initialized, you should immediately set up your selected announcement mechanism -- either subscribe to your push notifications or set up a thread to poll for updates.  </p> <p>Implementations should maintain a list of subscribed <code>HollowConsumer</code>s, and each time <code>subcribeToUpdates(HollowConsumer consumer)</code> is called, you should add the provided <code>HollowConsumer</code> to your list.  When the announced version changes, call <code>triggerAsyncRefresh()</code> on each subscribed consumer.</p> <p>Whether or not any <code>HollowConsumer</code>s are subscribed, implementations should return the latest announced version each time <code>getLatestVersion()</code> is called.</p> <p>HollowConsumer subscribes itself</p> <p>A <code>HollowConsumer</code> will automatically call <code>subscribeToUpdates()</code> with itself for an <code>AnnouncementWatcher</code> with which it is initialized.</p>"},{"location":"infrastructure/#pinning-consumers","title":"Pinning Consumers","text":"<p>Mistakes happen.  What's important is that we can recover from them quickly.  If you accidentally publish bad data, you should be able to revert those changes quickly.  If you give your <code>AnnouncementWatcher</code> implementation an alternate location to read the announcement from, which overrides the announcement from the consumer, then you can use this to quickly force clients to go back to any arbitrary state in the past.  We call setting a state version in this alternate location pinning the consumers.</p> <p>Implementing a pinning mechanism is extremely useful and highly recommended.  You can operationally reverse data issues immediately upon discovery, so that symptoms go away while you diagnose exactly what went wrong.  This can save an enormous amount of stress and money.</p> <p>Unpinning</p> <p>If you've pinned consumers due to a data issue, it's probably not desirable to simply 'unpin' them after the root cause is addressed.  Instead, restart the producer and instruct it to restore from the pinned state.  It should then produce a delta which skips over all of the bad states.  Only unpin after the delta from the pinned version to a bad version is overwritten with a delta from the pinned version to the good version.</p>"},{"location":"infrastructure/#blob-namespaces","title":"Blob Namespaces","text":"<p>Different use cases within your organization may want to reuse the same infrastructure integration.  You may want your <code>BlobRetriever</code> and <code>AnnouncementWatcher</code> to allow for multiple blob namespaces, one for each use case.</p>"},{"location":"interacting-with-a-dataset/","title":"Interacting with a Hollow Dataset","text":""},{"location":"interacting-with-a-dataset/#generated-object-api","title":"Generated Object API","text":"<p>Each of the examples provided thus far have focused on interaction with the Hollow data set via the generated Hollow Objects API.  </p> <p>Hollow Objects are instantiated at the time they are requested.  Each Hollow Object references a single record of a specific type, and holds two things:</p> <ul> <li>A reference to the Hollow data store for the type</li> <li>An ordinal</li> </ul> <p>A Hollow Object contains methods to retrieve each of its type's fields in the data model from which the API was generated.  Each time a field retrieval method is called on a Hollow Object, the data is retrieved directly from the Hollow data store.</p> <p>Hollow Objects are 'hollow'</p> <p>The name Hollow is derived from the fact that these objects are 'hollow' -- they appear to contain field accessors, but those are just facades which access the underlying data store.</p>"},{"location":"interacting-with-a-dataset/#generated-type-api","title":"Generated Type API","text":"<p>At times, using the Hollow Object API can result in a high rate of Object allocation.  All generated Hollow APIs also provide a way to interact with the data without creating Objects.  This is accomplished by using record ordinals to query the data store directly.  For example: <pre><code>MovieTypeAPI movieTypeAPI = movieAPI.getMovieTypeAPI();\nListOfActorTypeAPI listOfActorTypeAPI = movieAPI.getListOfActorTypeAPI();\nActorTypeAPI actorTypeAPI = movieAPI.getActorTypeAPI();\nStringTypeAPI stringTypeAPI = movieAPI.getStringTypeAPI();\n\nint movieOrdinal = movieIdx.getMatchingOrdinal(6);\nint listOfActorsOrdinal = movieTypeAPI.getActors(movieOrdinal);\n\nint numActors = listOfActorTypeAPI.size(listOfActorsOrdinal);\n\nfor(int i=0; i&lt;numActors; i++) {\n   int actorOrdinal = \n                 listOfActorTypeAPI.getElementOrdinal(listOfActorsOrdinal, i);\n   int stringOrdinal = actorTypeAPI.getActorNameOrdinal(actorOrdinal);\n   System.out.println(\"Starring \" + stringAPI.getValue(stringOrdinal));\n}\n</code></pre></p> <p>In extremely tight loops, it may be more efficient to use the Type API rather than the Object API.</p> <p>Avoid Premature Optimization</p> <p>In all but the tightest, most frequently executed loops, usage of the Type API will be unnecessary.  Its usage should be applied judiciously, since the pattern can be more difficult to maintain.</p>"},{"location":"interacting-with-a-dataset/#generic-object-api","title":"Generic Object API","text":"<p>Hollow also includes a generic Hollow Object API which, if sufficient for consumers, obviates the need to provide generated code: <pre><code>int movieOrdinal = movieIdx.getMatchingOrdinal(1);\n\nGenericHollowObject movie = new GenericHollowObject(readEngine, \"Movie\", movieOrdinal);\n\nString title = movie.getObject(\"title\").getString(\"value\");\n\nfor(GenericHollowObject actor : movie.getList(\"actors\").objects()) {\n    String actorName = actor.getObject(\"actorName\").getString(\"value\");\n    System.out.println(\"Starring \" + actorName);\n}\n</code></pre></p> <p>Working with the Generic Object API can become cumbersome \u2014 unlike a generated Hollow API, the IDE type assist cannot provide a guide to the data model.  However, for simple data models and explorational tasks the Generic Object API can be useful.</p>"},{"location":"license/","title":"License","text":"<p>Copyright 2016 Netflix, Inc.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"producer-consumer-apis/","title":"The Producer/Consumer APIs","text":"<p>In Getting Started we encountered basic usage of the <code>HollowProducer</code> and <code>HollowConsumer</code> APIs. This basic usage implies some default behavior which, if desired, may be customized to better suit your purposes.  A  more in-depth exploration of the available customizable features of these APIs follows.</p>"},{"location":"producer-consumer-apis/#the-hollowproducer","title":"The HollowProducer","text":"<p>Generally, a producer runs a repeating cycle.  At the end of each cycle, the producer has created a data state,  published the artifacts necessary for consumers to bring their in-memory data stores to that data state, and announced  the availability of the state.</p> <p>The <code>HollowProducer</code> encapsulates the details of publishing, announcing, validating, and (if necessary) rollback of data  states.  In order to accomplish this, a few infrastructure hooks should be injected:</p> <pre><code>HollowProducer\n   .withPublisher(publisher)         /// required: a BlobPublisher\n   .withAnnouncer(announcer)         /// optional: an Announcer\n   .withValidators(validators)       /// optional: one or more Validator\n   .withListeners(listeners)         /// optional: one or more HollowProducerListeners\n   .withBlobStagingDir(dir)          /// optional: a java.io.File\n   .withBlobCompressor(compressor)   /// optional: a BlobCompressor\n   .withBlobStager(stager)           /// optional: a BlobStager\n   .withSnapshotPublishExecutor(e)   /// optional: a java.util.concurrent.Executor\n   .withNumStatesBetweenSnapshots(n) /// optional: an int\n   .withTargetMaxTypeShardSize(size) /// optional: a long\n   .withBlobStorageCleaner(blobStorageCleaner) //optional: a BlobStorageCleaner\n   .withMetricsCollector(hollowMetricsCollector) //optional: a HollowMetricsCollector&lt;HollowProducerMetrics&gt;\n</code></pre> <p>Let's examine each of the injected configurations into the <code>HollowProducer</code>:</p> <ul> <li><code>BlobPublisher</code>: Implementations of this class define how to publish blob data to the blob store.</li> <li><code>Announcer</code>: Implementations of this class define the announcement mechanism, which is used to track the version of  the currently announced state.</li> <li><code>Validator</code>: Implementations of this class allow for semantic validation of the data contained in a state prior to  announcement.  If an Exception is thrown during validation, the state will not be announced, and the producer will be  automatically rolled back to the prior state.</li> <li><code>HollowProducerListener</code>: Listeners are notified about the progress and status of producer cycles throughout the  various cycle stages.</li> <li>Blob staging directory: Before blobs are published, they must be written and inspected/validated.  A directory may  be specified as a File to which these \"staged\" blobs will be written prior to publish.  Staged blobs will be cleaned up  automatically after publish.</li> <li><code>BlobCompressor</code>: Implementations of this class intercept blob input/output streams to allow for compression in the  blob store.</li> <li><code>BlobStager</code>: Implementations will define how to stage blobs, if the default behavior of staging blobs on local disk  is not desirable.  If a custom <code>BlobStager</code> is provided, then neither a blob staging directory or <code>BlobCompressor</code>  should be provided.</li> <li>Snapshot publish <code>Executor</code>: When consumers start up, if the latest announced version does not have a snapshot,  they can load an earlier snapshot and follow deltas to get up-to-date.  A state can therefore be available and announced  prior to the availability of the snapshot.  If an Executor is supplied here, then it will be used to publish snapshots. This can be useful if snapshot publishing takes a long time -- subsequent cycles may proceed while snapshot uploads are  still in progress.</li> <li>Number of cycles between snapshots: Because snapshots are not necessary for a data state to be available and  announced, they need not be published every cycle.  If this parameter is specified, then a snapshot will be produced  only every <code>(n+1)th</code> cycle.</li> <li><code>VersionMinter</code>: Allows for a custom version identifier minting strategy.</li> <li>Target max type shard size: Specify a target max type shard size.  Defaults to  16MB.</li> <li><code>BlobStorageCleaner</code>: Using the blob storage cleaning capability, it's  possible to free up the blob storage and prevent running out of space because of old snapshots/deltas.</li> <li><code>HollowMetricsCollector</code>: Implementing a <code>HollowMetricsCollector</code> allows to either store or  publish those metrics to your preferred provider, such as Prometheus.</li> </ul> <p>Each time a new data state should be produced, users should call <code>.runCycle(Populator)</code>.  See  Getting Started for more basic usage details.</p>"},{"location":"producer-consumer-apis/#restoring-at-startup","title":"Restoring At Startup","text":"<p>Ideally the same <code>HollowProducer</code> would be held in memory forever, and <code>runCycle()</code> would be called every so often to  produce a never-ending intact delta chain.  However, this isn\u2019t always possible; the producer will need to be  restarted from time to time due to deployment or other operational circumstances.</p> <p>In order to produce a delta between states produced by one <code>HollowProducer</code> and another, the producer can restore the  prior state upon restart, which will allow a delta and reverse delta to be produced.  See  Restoring at Startup for usage.</p> <p>Once we have restored the prior state, we can produce a delta from our producer's first cycle.  The delta will be  applicable to any consumers which are on the state from which we restored.  </p> <p>Initializing Before Restore</p> <p>Before restoring, we must always initialize our data model.  A <code>HollowProducer</code>'s data model may be initialized:</p> <ul> <li>via the <code>HollowObjectMapper</code> by calling <code>initTypeState()</code> with all top-level classes</li> <li>via a set of schemas loaded from a text file using the <code>HollowSchemaParser</code>  and <code>HollowWriteStateCreator</code></li> </ul> <p>Truncating a Delta Chain</p> <p>If a problem occurs and you need to pin back consumers, you may want to restart your  producer and explicitly restore from the pinned state.  Once the producer's first cycle completes, it will publish a  delta from the pinned state to the newly produced state, overwriting the previous delta from the pinned state.  In  this way, when you unpin, consumers will automatically follow the new delta, and the old forward-path from the  pinned state will be truncated.</p> <p>If any consumers somehow did happen to remain on a truncated state, the reverse delta out of the truncated chain  is still intact -- they could be manually pinned back to the restored state, then unpinned to get back up-to-date.</p>"},{"location":"producer-consumer-apis/#rolling-back","title":"Rolling Back","text":"<p>While producing a new state, if the <code>HollowProducer</code> encounters an error during data state population or validation  fails, the current data state will be aborted and the underlying state engine will be rolled back to the previous  data state.  Any delta produced on the next cycle will be from the last successful data state.</p>"},{"location":"producer-consumer-apis/#validating-data","title":"Validating Data","text":"<p>It likely makes sense to perform some basic validation on your produced data states before announcing them to clients. If you provide one or more <code>Validator</code>s to the <code>HollowProducer</code>, these will be automatically executed prior to  announcement.  Validation rules will be specific to the semantics of the dataset, and may include some heuristics-based  metrics based on expectations about the dataset.  If your <code>Validator</code> throws an Exception, the <code>HollowProducer</code> will  automatically roll back the state engine and the next successful cycle will produce a delta from the prior successful  state.  </p>"},{"location":"producer-consumer-apis/#compacting-data","title":"Compacting Data","text":"<p>It is possible to produce delta chains which extend over many thousands of states.  If during this delta chain an  especially large delta happens for a specific type, it\u2019s possible that many ordinal holes will be present in that type. If over time multiple types go through especially large deltas, this can have an impact on a dataset\u2019s heap footprint.</p> <p>To reclaim heap space occupied by ordinal holes, a special compaction cycle can be run on the <code>HollowProducer</code>. During compaction, no record data will change, but identical records will be relocated off of the high end of the  ordinal space into the ordinal holes.  This is accomplished by producing a new data state with no changes except for the  more optimal ordinal assignments.</p> <p>To run a compaction cycle, call <code>runCompactionCycle(config)</code> on the <code>HollowProducer</code>.  If this method returns a valid  version identifier, then a compaction cycle occurred and produced a new data state.  If it returns <code>Long.MIN_VALUE</code>,  then the compaction criteria specified in the <code>CompactionConfig</code> was not met and no action was taken.  See the  <code>HollowCompactor</code> javadoc for more details.</p>"},{"location":"producer-consumer-apis/#the-incremental-hollowproducer","title":"The Incremental HollowProducer","text":"<p>If it is known what changes are to be applied to a data state then an incremental producer can utilized.  Instead of providing the whole data state (except initially on the first production of the datastate), all records, on each  cycle the set of records that have changed may be provided (those records added, modified, or removed).  Each record  must be associated with a primary type (the record's Java type must be annotated with <code>@HollowPrimaryKey</code>) to ensure the  record's identity can be determined and therefore calculate the change.</p> <p>In all other respects an incremental HollowProducer behaves the same, encapsulating the details of publishing,  announcing, validating, and (if necessary) rollback of data states, with the same injection of infrastructure hooks when building.  Consumers are non-the-wiser as to full or incremental production.</p> <p>An incremental HollowProducer may be built in the same manner as a HollowProducer as follows:</p> <pre><code>HollowProducer.Incremental ip = HollowProducer\n   .withPublisher(publisher)         \n   ... // further infrastructure hooks\n   buildIncremental();\n</code></pre> <p>Primary key records</p> <p>It is recommend that any top-level type be annotated with <code>@HollowPrimaryKey</code> regardless of full or incremental production since this makes it easy for tooling to track changes.</p>"},{"location":"producer-consumer-apis/#the-hollowconsumer","title":"The HollowConsumer","text":"<p>Data consumers keep their local copy of a dataset current by ensuring that their state engine is always at the latest  announced data state. Consumers can arrive at a particular data state in a couple of different ways:</p> <ul> <li>At initialization time, they will load a snapshot, which is an entire copy of the dataset to be forklifted into memory.</li> <li>After initialization time, they will keep their local copy of the dataset current by applying delta transitions, which  are the differences between adjacent data states.</li> </ul> <p>The <code>HollowConsumer</code> encapsulates the details of initializing and keeping a dataset up to date.  In order to accomplish  this task, a few infrastructure hooks should be injected:</p> <pre><code>HollowConsumer\n   .withBlobRetriever(blobRetriever)              /// required: a BlobRetriever\n   .withLocalBlobStore(localDiskDir)              /// optional: a local disk location\n   .withAnnouncementWatcher(announcementWatcher)  /// optional: a AnnouncementWatcher\n   .withRefreshListener(refreshListener)          /// optional: a RefreshListener\n   .withGeneratedAPIClass(MyGeneratedAPI.class)   /// optional: a generated client API class\n   .withFilterConfig(filterConfig)                /// optional: a HollowFilterConfig\n   .withDoubleSnapshotConfig(doubleSnapshotCfg)   /// optional: a DoubleSnapshotConfig\n   .withObjectLongevityConfig(objectLongevityCfg) /// optional: an ObjectLongevityConfig\n   .withObjectLongevityDetector(detector)         /// optional: an ObjectLongevityDetector\n   .withRefreshExecutor(refreshExecutor)          /// optional: an Executor\n   .withMetricsCollector(hollowMetricsCollector) //optional: a HollowMetricsCollector&lt;HollowConsumerMetrics&gt;\n   .build();\n</code></pre> <p>Let's examine each the injected hooks to the <code>HollowConsumer</code>:</p> <ul> <li><code>BlobRetriever</code>: The interface to the blob store.  This is the only hook for which a custom implementation is  required.  Each of the other hooks have default implementations which may be used.  The <code>BlobRetriever</code> may be omitted  only if a previously-populated local blob store is specified.</li> <li>Local blob store: A <code>File</code> which indicates where to record downloaded blobs and find previously downloaded blobs. If specified along with a <code>BlobRetriever</code>, the <code>HollowConsumer</code> will prefer to use previously downloaded blobs where  applicable, and otherwise write newly downloaded blobs to the specified directory.  If specified without a  <code>BlobRetriever</code>, only previously downloaded blobs will be available.</li> <li><code>AnnouncementWatcher</code>: Provides an interface to the state announcement mechanism.  Often, announcement polling logic  is encapsulated inside implementations.</li> <li><code>RefreshListener</code>: Provides hooks so that actions may be taken during and after updates (e.g. indexing).</li> <li>Generated API Class: Specifies a custom-generated Hollow API to use.</li> <li><code>HollowFilterConfig</code>:</li> <li><code>DoubleSnapshotConfig</code>: Defines advanced settings related to double snapshots.</li> <li><code>ObjectLongevityConfig</code>: Defines advanced settings related to object longevity.</li> <li><code>ObjectLongevityDetector</code>: Implementations are notified when stale hollow object existence and usage is detected.</li> <li><code>RefreshExecutor</code>: An <code>Executor</code> to use when asynchronous updates are called via <code>triggerAsyncRefresh()</code>.</li> <li><code>HollowMetricsCollector</code>: Implementing a <code>HollowMetricsCollector</code> allows to either store or  publish those metrics to your preferred provider, such as Prometheus.</li> </ul> <p>Each time the identifier of the currently announced state changes, <code>triggerRefresh()</code> should be called on the  <code>HollowConsumer</code>.  This will bring the data up to date.</p> <p>In general, the only requirement for getting Hollow consumers to work with your specific infrastructure is to implement  a <code>BlobRetriever</code> and <code>AnnouncementWatcher</code>, and use them with a <code>HollowConsumer</code>.</p> <p>Triggering Refresh</p> <p>When implementing a <code>AnnouncementWatcher</code>, you will need to implement the method  <code>subscribeToUpdates(HollowConsumer consumer)</code>.  When you create a <code>HollowConsumer</code> with an <code>AnnouncementWatcher</code>, it  will automatically call back to this method with itself as the argument.  </p> <p>You should track all <code>HollowConsumer</code>s received by calls to this method.  When your announcement mechanism provides  an updated value, you should notify each <code>HollowConsumer</code> via the <code>triggerAsyncRefresh()</code> method.</p> <p>In this way, your <code>HollowConsumer</code> injected with this <code>HollowAnnouncementWatcher</code> implementation will be  automatically kept up-to-date.</p>"},{"location":"producer-consumer-apis/#dataset-consistency","title":"Dataset Consistency","text":"<p>If you have a long-running process which requires a consistent view of the dataset in a single state, you can prevent  the <code>HollowConsumer</code> from updating while your process runs:</p> <pre><code>HollowConsumer consumer = ...\n\nconsumer.getRefreshLock().lock();\ntry {\n    /// run your process\n} finally {\n    consumer.getRefreshLock().unlock();\n}\n</code></pre> <p>The <code>getRefreshLock()</code> call returns the read lock in a <code>ReadWriteLock</code>.  Refreshes use the write lock.</p>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Hollow has an available reference implementation, which is designed to get you up and running with a demo in minutes.  Then, we'll walk through swapping in a fully functional, production scalable, AWS-based infrastructure implementation in about an hour.</p> <p>The reference implementation is a great starting point to integrate Hollow for your use case; it contains a simple mocked-up data model, which you can easily modify to suit your needs.</p> <p>Learn by Doing</p> <p>This Quick Start Guide is placed at the beginning of the documentation as a starting point for those who prefer to \"learn by doing\".  If you'd prefer to gain a greater understanding of how everything fits together prior to jumping in, skip ahead to the Getting Started guide, but do come back later.  </p>"},{"location":"quick-start/#running-the-demo","title":"Running the Demo","text":""},{"location":"quick-start/#clone-the-reference-implementation","title":"Clone the Reference Implementation","text":"<p>Start by cloning the netflix-hollow-reference-implementation repo on GitHub:</p> <pre><code>git clone https://github.com/Netflix/hollow-reference-implementation.git\n</code></pre>"},{"location":"quick-start/#import-into-your-ide","title":"Import into your IDE","text":"<p>Import the project into your IDE.  This project ships with both a <code>build.gradle</code> file and a <code>pom.xml</code> file, so you can either use a gradle plugin or a standard maven plugin to import the dependencies.</p> <p>Dependencies in Hollow</p> <p>The core Hollow jar does not require or include any third party dependencies.  The dependencies in the reference implementation are required for infrastructure demonstration purposes.</p>"},{"location":"quick-start/#start-a-producer","title":"Start a Producer","text":"<p>The class <code>how.hollow.producer.Producer</code> contains a main method.  Run it.  This will be our data producer, and will write data to a directory publish-dir underneath the temp directory.</p> <p>You should see output like this: <pre><code>I AM THE PRODUCER.  I WILL PUBLISH TO /tmp/publish-dir\nATTEMPTING TO RESTORE PRIOR STATE...\nRESTORE NOT AVAILABLE\n...\n</code></pre></p> <p>And you should have a folder <code>/tmp/publish-dir</code> with contents like this:</p> <pre><code>    17 announced.version\n   370 delta-20161110185218001-20161110185228002\n   604 delta-20161110185228002-20161110185238003\n   385 reversedelta-20161110185228002-20161110185218001\n   551 reversedelta-20161110185238003-20161110185228002\n597567 snapshot-20161110185218001\n597688 snapshot-20161110185228002\n597813 snapshot-20161110185238003\n</code></pre>"},{"location":"quick-start/#start-a-consumer","title":"Start a Consumer","text":"<p>The class <code>how.hollow.consumer.Consumer</code> also contains a main method.  Run it.  This will be our data consumer, and will read data from the directory publish-dir underneath the temp directory.  </p> <p>You should see output like this: <pre><code>I AM THE CONSUMER.  I WILL READ FROM /tmp/publish-dir\nSNAPSHOT COMPLETED IN 45ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 20ms\nTYPES: [Movie, SetOfActor, String]\nSNAPSHOT COMPLETED IN 1ms\nTYPES: [Actor, Movie]\nDELTA COMPLETED IN 12ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 0ms\nTYPES: [Movie]\n</code></pre></p>"},{"location":"quick-start/#inspecting-the-data","title":"Inspecting the Data","text":"<p>That's it!  You now have a producer, which is reading data from a data source and publishing Hollow data to <code>/tmp/publish-dir</code>, and a consumer which is reading that data and keeping its internal state up to date.  Let's take a look at what the data actually looks like, and how it's changing over time.  The consumer started a history server, so open up a browser and visit http://localhost:7777.</p> <p>Fake Data</p> <p>In this demo case, there is no data source.  Instead, the producer is manufacturing some randomized fake data on the first cycle, and making some minor random modifications to that data for each cycle.  Inspect the package <code>how.hollow.producer.datamodel</code> to see the mock data model and how it is randomized.</p>"},{"location":"quick-start/#plugging-in-infrastructure","title":"Plugging in Infrastructure","text":"<p>The demo we have so far is fully functional, but writing to and reading from a local disk directory is probably not realistic for a production implementation.  In the following few sections, we'll actually set up a simple AWS-based infrastructure which could easily scale to any production deployment.</p> <p>Using AWS</p> <p>This demonstration uses AWS because it is accessible to anyone, easy to set up, and extremely reliable.  In order to proceed, you'll need an AWS account, which is free to sign up for -- you only pay for the services you use.</p> <p>Even if you're not able to use the prescribed AWS-based infrastructure, running through the following steps will be useful to gain an understanding of how to swap in different infrastructure implementations for use with Hollow.</p>"},{"location":"quick-start/#create-a-user","title":"Create a User","text":"<p>Once you've logged into your AWS account, select Identity &amp; Access Management from the AWS console:</p> <p></p> <p>Select Users, then Add User.  Enter a name in box (e.g. HollowReference).</p> <p></p> <p>Select the checkbox Programmatic access.  Click Next:Permissions in the bottom right corner of the screen.</p> <p>On the next page, Click Attach existing policies directly.  </p> <p></p> <p>We'll need S3 and DynamoDB access for this user.  For now, let's give this user full access to S3.  Select the checkbox next to the policy named AmazonS3FullAccess:</p> <p></p> <p>Using the same interface, after the S3 policy checkbox is selected, on the same page search for and select the AmazonDynamoDBFullAccess policy.</p> <p>Click Next: Review.  You should see the following:</p> <p></p> <p>Click Create User.  Here you'll see an <code>Access Key ID</code> and a <code>Secret Access Key</code>.  Copy both of these strings and set them off to the side, we'll need them soon.</p>"},{"location":"quick-start/#create-an-s3-bucket","title":"Create an S3 Bucket","text":"<p>Back on the AWS console landing page, select S3:</p> <p></p> <p>Click Create Bucket, then create a bucket.  Select a unique name for your bucket.  Your bucket can be in any region, here we're using the US Standard region:</p> <p></p> <p>Click Next.</p> <p>Leave the defaults on the next couple of screens for now.  Click Next twice more, then Create Bucket.</p>"},{"location":"quick-start/#plug-the-producer-into-s3","title":"Plug the Producer into S3","text":"<p>Now that we've set up our user and our S3 bucket, we can plug the producer into S3.  Open up the <code>Producer</code> class in the Hollow reference implementation project.  Modify the <code>main</code> method: swap the <code>Publisher</code> and <code>Announcer</code> implementations as follows, replacing <code>zzz-hollow-reference-implementation</code> with the bucket name you chose in the prior step:</p> <pre><code>AWSCredentials credentials =\n                new BasicAWSCredentials(\"&lt;Access Key ID&gt;\", \"&lt;Secret Access Key&gt;\");\n\nPublisher publisher =\n                new S3Publisher(credentials, \"zzz-hollow-reference-implementation\", \"demo\");\nAnnouncer announcer =\n                new S3Announcer(credentials, \"zzz-hollow-reference-implementation\", \"demo\");\n\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .build();\n</code></pre> <p>Start the producer.  After a few cycles, inspect the <code>demo</code> folder in your S3 bucket to see what the producer is writing.</p> <p>Watch your AWS Usage</p> <p>The AWS meter is running.  Leaving this version of the producer running overnight could run up an AWS bill.  Shut the demo down once you're done!</p> <p>Clean Up After Yourself</p> <p>If you do ultimately choose to use S3 as your blob storage infrastructure, be aware that this implementation does not automatically clean up data for you, which can result in increasing AWS bills.  You should have a cleanup strategy, which can be as simple as adding a Lifecycle Rule to your bucket which will, for example, delete old data after 30 days.</p>"},{"location":"quick-start/#plug-the-consumer-into-s3","title":"Plug the Consumer into S3","text":"<p>Now that our producer is writing data into S3, we need our consumers to read that data.  Open up the <code>Consumer</code> class in the Hollow reference implementation project.  Modify the <code>main</code> method: swap the <code>BlobRetriever</code> and <code>AnnouncementWatcher</code> implementations as follows, replacing <code>zzz-hollow-reference-implementation</code> with the bucket name you chose in the prior step:</p> <pre><code>AWSCredentials credentials =\n                new BasicAWSCredentials(\"&lt;Access Key ID&gt;\", \"&lt;Secret Access Key&gt;\");\n\nBlobRetriever blobRetriever =\n        new S3BlobRetriever(credentials, \"zzz-hollow-reference-implementation\", \"demo\");\nAnnouncementWatcher announcementWatcher =\n        new S3AnnouncementWatcher(credentials, \"zzz-hollow-reference-implementation\", \"demo\");\n\nConsumer consumer = HollowConsumer.withBlobRetriever(blobRetriever)\n                                  .withAnnouncementWatcher(announcementWatcher)\n                    ...\n                                  .build();\n</code></pre> <p>Start the consumer.  You'll see the same output from the demo step, except now the Hollow input data is coming from S3.  </p> <p>At this point, we have a fully distributed infrastructure.  You can start the producer on one machine, and start a consumer on many other machines, anywhere in the world.  Each of the consumers will update in lock-step each time the producer publishes changes.</p> <p>Address already in use Exception</p> <p>Note that the <code>Consumer</code> attempts to start a history server on port 7777.  Because of this, only one <code>Consumer</code> can be running on a single machine at one time.  If you get a <code>java.net.BindException</code>, shut down the other <code>Consumer</code> and try again.</p> <p>Publishing Closer to the Consumer</p> <p>Our implementation currently publishes to a single S3 bucket.  This is OK, but S3 buckets reside in specific AWS regions, and it is often beneficial to publish data closer to your consumers.  For example, if you have some consumers in the AWS region US Standard, and some in the region Sydney, then it makes sense to simultaneously publish to one bucket in each region, and have consumers read from the bucket closest to them.</p>"},{"location":"quick-start/#a-better-announcement-infrastructure","title":"A Better Announcement Infrastructure","text":"<p>Our distributed infrastructure thus far leverages S3 for both a blob store mechanism, and an announcement mechanism.  Although S3 is perfectly suitable for blob storage, it's less well suited for announcement.  Instead, we can leverage DynamoDB for the announcement infrastructure, and achieve both improved scalability and better economics.</p> <p>First, we need to create a DynamoDB table.  Back on the AWS console landing page, select DynamoDB:</p> <p></p> <p>Select Create table and enter a Table name (e.g. HollowAnnouncement) and use namespace as the Partition key:</p> <p></p> <p>Select Create.</p>"},{"location":"quick-start/#plug-the-producer-into-dynamodb","title":"Plug the Producer into DynamoDB","text":"<p>Now that we've set up our DynamoDB table, let's swap it into our producer as our announcement mechanism.  Go back to the <code>Producer</code> class and modify the <code>main</code> method, swapping the <code>Announcer</code> implementation as follows:</p> <pre><code>Announcer announcer = new DynamoDBAnnouncer(credentials, \"HollowAnnouncement\", \"demo\");\n</code></pre> <p>Start the producer.  After at least one cycle, you'll be able to scan the table and see a record indicating the currently announced version for our <code>demo</code> namespace:</p> <p></p>"},{"location":"quick-start/#plug-the-consumer-into-dynamodb","title":"Plug the Consumer into DynamoDB","text":"<p>Now that our producer is announcing to DynamoDB, of course our consumer needs to look there for update directions.  Go back to the <code>Consumer</code> class and modify the <code>main</code> method, swapping the <code>AnnouncementWatcher</code> implementation as follows:</p> <pre><code>AnnouncementWatcher announcementWatcher =\n            new DynamoDBAnnouncementWatcher(credentials, \"HollowAnnouncement\", \"demo\");\n</code></pre> <p>Run the consumer.  You'll see the same output from prior steps, except now the Hollow input data is coming from S3, and the state version announcement is read from DynamoDB.</p> <p>Pinning The State</p> <p>The provided <code>DynamoDBAnnouncementWatcher</code> can be pinned.  If the announcement item in DynamoDB contains a field <code>pin_version</code>, then the consumer will go to the version indicated in that field, instead of the version the producer announces.  See Pinning Consumers for more details about pinning data.</p>"},{"location":"quick-start/#plug-in-your-data-model","title":"Plug In Your Data Model","text":"<p>Congratulations!  You now have a living, breathing, fully production scalable implementation of Hollow in your hands.  All that's left to do is substitute your data model for the example provided.  The data model can be defined, as in this example, with a set of POJO classes.  Take this starting point and make it your own -- remove the classes underneath <code>how.hollow.producer.datamodel</code> and replace them with your data model.</p> <p>Modeling Data with POJOs</p> <p>The <code>HollowObjectMapper</code> section of this document provides details about how to convert your POJO data model into Hollow.</p> <p>Continue on to the Getting Started guide to learn more about how the fundamental pieces of Hollow fit together, how to create a consumer API custom-tailored to your data model, and how to index your data for easy, efficient retrieval by consumers.</p>"},{"location":"testing/","title":"Unit testing with Hollow objects","text":"<p>Hollow provides a <code>hollow-test</code> jar that contains some testing utilities to facilitate unit testing with Hollow objects.</p>"},{"location":"testing/#hollowreadstateenginebuilder","title":"HollowReadStateEngineBuilder","text":"<p>This class allows easy creation of <code>HollowReadStateEngine</code> objects for use in unit tests. This is useful when paired with a <code>TestHollowConsumer</code> (see below), by itself while using lower-level Hollow APIs, or as a mechanism to create Hollow objects for use in test cases.</p> <pre><code>HollowReadStateEngine engine = new HollowReadStateEngineBuilder()\n    .add(new Movie(\"foo\"))\n    .add(new Actor(\"bar\"))\n    .build()\n</code></pre>"},{"location":"testing/#testhollowconsumer","title":"TestHollowConsumer","text":"<p>A common pattern during unit or integration tests is to require an instance of a HollowConsumer that can be manipulated, similar to how test fixtures or mocks are manipulated. <code>hollow-test</code> provides a <code>TestHollowConsumer</code> class to facilitate this.</p> <p>A simple example is shown below. See the unit tests in <code>TestHollowConsumerTest</code> for further examples.</p> <pre><code>HollowWriteStateEngine stateEngine = new HollowWriteStateEngineBuilder()\n    .add(\"somedata\")\n    .add(new MyDataModelType(\"somestuff\", 2))\n    .build();\n// we will add the snapshot with a version, and make the announcementWatcher see this version\nlong latestVersion = 1L;\nTestHollowConsumer consumer = new TestHollowConsumer.Builder()\n       .withAnnouncementWatcher(new TestAnnouncementWatcher().setLatestVersion(latestVersion))\n       .withBlobRetriever(new TestBlobRetriever())\n       .withGeneratedAPIClass(MyApiClass.class)\n       .build();\nconsumer.addSnapshot(latestVersion, stateEngine);\nconsumer.triggerRefresh();\n</code></pre>"},{"location":"testing/#hollowtestdataapigenerator","title":"HollowTestDataAPIGenerator","text":"<p>Hollow Test Data API eases the creation of dummy data for testing. </p> <p>The Test API can be generated using the <code>HollowTestDataAPIGenerator</code> class by passing in a <code>HollowDataset</code> or using the data model like- </p> <p><pre><code>HollowTestDataAPIGenerator.generate(\n    dataset,\n    \"some.package.name\", \n    \"InputTestData\",\n    \"/path/to/generated/sources\");\n</code></pre> where <code>dataset</code> could be a snapshot published by a prior producer run if available, or it could be initialized from the  data model classes by doing <code>SimpleHollowDataset.fromClassDefinitions(Movie.class)</code> where <code>Movie.class</code> is the  top-level Class in the data model. In this example an instance of the generated class <code>InputTestData</code> can then be used  to construct a state for the consumer with dummy data for the test for e.g.- <pre><code>InputTestData input = new InputTestData();\ninput\n    .Movie() \n        .id(1L)\n        .title(\"foo\")\n        .countries() // is a set \n            .String(\"US\")\n            .String(\"CA\")\n            .up() // to parent\n        .tags() // is a map\n            .entry(TYPE, \"Movie\")\n            .entry(GENRE, \"action\");\n</code></pre></p> <p>And a <code>HollowConsumer</code> can be initialized with the data state like- <pre><code>HollowConsumer consumer = input\n    .newConsumerBuilder()\n    .withGeneratedAPIClass(ClientAPI.class) // the client API used to access data in a Hollow Consumer and not the test data generator API that was generated in this example\n    .build();\ninput.buildSnapshot(consumer);\n</code></pre></p>"},{"location":"testing/#fake-dataset","title":"Fake dataset","text":"<p>A pre-generated fake dataset is available for download publicly in an AWS S3 bucket.  Alternatively, the code for generating that dataset is available under the <code>hollow-fakedata</code> module of this repo and it  can be run locally to produce a fake dataset for testing. There are some instructions in the README. </p> <p>For reusing the pre-generated fake dataset, download from S3 to your local machine by installing the aws cli and running this command: <pre><code>aws s3 cp s3://hollow-oss-public/fakedata /tmp/download --recursive  --no-sign-request\n</code></pre> This will download around 500MB of data comprising a 100 version delta chain for a fake Book catalog.  These Hollow snapshot and delta blobs can be consumed in your test using the various Hollow APIs available, for e.g.  this code can consume those blobs and run the hollow history server over that delta chain:  <pre><code>@Test\npublic void fakeHistory() throws Exception {\n    String path = \"/tmp/fakedata\";  // path to downloaded blobs\n    HollowConsumer c = HollowConsumer\n        .withBlobRetriever(new HollowFilesystemBlobRetriever(Paths.get(path)))\n        .withDoubleSnapshotConfig(new HollowConsumer.DoubleSnapshotConfig() {\n            @Override\n            public boolean allowDoubleSnapshot() {\n                    return false;   // disable double snapshots to get history for each state in delta chain\n                }\n            @Override                \n            public int maxDeltasBeforeDoubleSnapshot() { \n                    return Integer.MAX_VALUE;\n                }\n            })\n        .build();\n    c.triggerRefreshTo(20230808144752001l);     // first version in the delta chain\n    HollowHistoryUIServer historyUIServer = new HollowHistoryUIServer(c, 7001);\n    historyUIServer.start();\n    System.out.println(\"History server started at http://localhost:7001/\"); // open in browser to see history of changes\n\n    c.triggerRefreshTo(20230808145930100l);     // 100th version in delta chain, apply delta transitions and computing history for each state \n\n    historyUIServer.join(); // block forever\n}\n</code></pre></p>"},{"location":"tooling/","title":"Insight Tools","text":"<p>Once your data is Hollow, you will be able to gain better insights into it.  Hollow ships with a number of useful tools for quickly gaining insights into your data, from broad patterns at high level, to zooming in to find and inspect specific individual records.</p>"},{"location":"tooling/#metrics","title":"Metrics","text":"<p>Implementing a <code>HollowMetricsCollector</code> allows to retrieve metrics from your data. Hollow collects metrics for producer and consumers by default which can be accessed by the <code>HollowMetricsCollector</code> on every cycle.</p> <p>Common metrics for producer and consumer:</p> <ul> <li><code>typeHeapFootprint</code>: provides the approximate heap footprint for each Type in your data. This is based on Heap Usage Analysis</li> <li><code>typePopulatedOrdinals</code>: provides the number or ordinals per Type in your data.</li> <li><code>currentVersion</code>: provides the current version of your data.</li> <li><code>totalHeapFootprint</code>: provides the approximate heap footprint for your data. This is based on Heap Usage Analysis</li> <li><code>totalPopulatedOrdinals</code>: provides the total number of ordinals in your data.</li> </ul>"},{"location":"tooling/#producer","title":"Producer","text":"<p>In addition to the common metrics, hollow collects the following metrics from a producer:</p> <ul> <li><code>cyclesCompleted</code></li> <li><code>cyclesSucceeded</code></li> <li><code>cycleFailed</code> </li> <li><code>snapshotsCompleted</code> </li> <li><code>snapshotsFailed</code> </li> <li><code>deltasCompleted</code> </li> <li><code>deltasFailed</code> </li> <li><code>reverseDeltasCompleted</code> </li> <li><code>reverseDeltasFailed</code> </li> </ul>"},{"location":"tooling/#usage","title":"Usage","text":"<pre><code>HollowProducer producer = HollowProducer.withPublisher(publisher)\n                                                .withMetricsCollector(hollowMetricsCollector) \n                                                .build();\n</code></pre>"},{"location":"tooling/#prometheus-example","title":"Prometheus Example","text":"<pre><code>import com.netflix.hollow.api.metrics.HollowMetricsCollector;\nimport com.netflix.hollow.api.metrics.HollowProducerMetrics;\nimport io.prometheus.client.Gauge;\n\nclass MyPrometheusProducerMetricsCollector extends HollowMetricsCollector&lt;HollowProducerMetrics&gt; {\n    private static Gauge hollowConsumerGauge = Gauge\n            .build()\n            .name(\"hollowProducerMetrics\")\n            .labelNames(\"metric\")\n            .help(\"Hollow Producer Metrics\")\n            .register();\n\n\n    private static void set(String metric, Double value) {\n        hollowConsumerGauge.labels(metric).set(value);\n    }\n\n    private static final long kb = 1024;\n\n    @Override\n    void collect(HollowConsumerMetrics metrics) {\n        //Common hollow metrics\n        metrics.getTypeHeapFootprint().forEach((String type, Long heapCost) -&gt;\n                set(type + \"_heap_cost_kb\", heapCost / kb)\n        );\n        metrics.getTypePopulatedOrdinals().forEach((String type, Long ordinals)-&gt;\n                set(type + \"_domain_object_count\", ordinals)\n        );\n        set(\"total_heap_cost_kb\", metrics.getTotalHeapFootprint());\n        set(\"total_domain_object_count\", metrics.getTotalPopulatedOrdinals());\n        set(\"current_version\", metrics.getCurrentVersion());\n\n        //Producer specific metrics\n        set(\"cycles_completed\", metrics.getCyclesCompleted());\n        set(\"cycles_succeeded\", metrics.getCyclesSucceeded());\n        set(\"cycles_failed\", metrics.getCycleFailed());\n        set(\"snapshots_completed\", metrics.getSnapshotsCompleted());\n        set(\"snapshotsFailed\", metrics.getSnapshotsFailed());\n        set(\"deltas_completed\", metrics.getDeltasCompleted());\n        set(\"deltas_failed\", metrics.getDeltasFailed());\n    }\n}\n</code></pre>"},{"location":"tooling/#consumer","title":"Consumer","text":"<p>In addition to the common metrics, hollow collects the following metrics from a consumer:</p> <ul> <li><code>refreshFailed</code></li> <li><code>refreshSucceeded</code></li> </ul>"},{"location":"tooling/#usage_1","title":"Usage","text":"<pre><code>HollowConsumer\n   .withBlobRetriever(blobRetriever)           \n   .withMetricsCollector(hollowMetricsCollector) \n   .build();\n</code></pre>"},{"location":"tooling/#prometheus-example_1","title":"Prometheus Example","text":"<pre><code>import com.netflix.hollow.api.metrics.HollowMetricsCollector;\nimport com.netflix.hollow.api.metrics.HollowConsumerMetrics;\nimport io.prometheus.client.Gauge;\n\nclass MyPrometheusConsumerMetricsCollector extends HollowMetricsCollector&lt;HollowConsumerMetrics&gt; {\n    private static Gauge hollowConsumerGauge = Gauge\n            .build()\n            .name(\"hollowConsumerMetrics\")\n            .labelNames(\"metric\")\n            .help(\"Hollow Consumer Metrics\")\n            .register();\n\n\n    private static void set(String metric, Double value) {\n        hollowConsumerGauge.labels(metric).set(value);\n    }\n\n    private static final long kb = 1024;\n\n    @Override\n    void collect(HollowConsumerMetrics metrics) {\n        //Common hollow metrics\n        metrics.getTypeHeapFootprint().forEach((String type, Long heapCost) -&gt;\n                set(type + \"_heap_cost_kb\", heapCost / kb)\n        );\n        metrics.getTypePopulatedOrdinals().forEach((String type, Long ordinals)-&gt;\n                set(type + \"_domain_object_count\", ordinals)\n        );\n        set(\"total_heap_cost_kb\", metrics.getTotalHeapFootprint());\n        set(\"total_domain_object_count\", metrics.getTotalPopulatedOrdinals());\n        set(\"current_version\", metrics.getCurrentVersion());\n\n        //Consumer specific metrics\n        set(\"refresh_failed\", metrics.getRefreshFailed());\n        set(\"refresh_succeeded\", metrics.getRefreshSucceded());\n    }\n}\n</code></pre>"},{"location":"tooling/#blob-storage-manipulation","title":"Blob Storage Manipulation","text":""},{"location":"tooling/#cleaning","title":"Cleaning","text":"<p>Using the <code>BlobStorageCleaner</code> capability, it's possible to free up the blob storage and prevent running out of space because of old snapshots/deltas.</p> <p>Hollow ships with <code>HollowFilesystemBlobStorageCleaner</code> which allows to only keep a given number of snapshots. It can be used in the following way:</p> <pre><code>HollowProducer.Publisher publisher = new HollowFilesystemPublisher(publishDir);\nHollowProducer.BlobStorageCleaner blobStorageCleaner = new HollowFilesystemBlobStorageCleaner(publishDir, 10);\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                                .withBlobStorageCleaner(blobStorageCleaner)\n                                                .build();\n</code></pre> <p>The integer <code>10</code> parameter will set the value for <code>numOfSnapshotsToKeep</code>.</p> <p>It is also possible to add your own blob storage cleaner by extending <code>HollowProducer.BlobStorageCleaner</code> class and provide your own mechanism to clean snapshots/deltas/reverse deltas.</p>"},{"location":"tooling/#hollow-explorer","title":"Hollow Explorer","text":"<p>Hollow ships with a UI which can be used to browse and search records within any dataset.</p>"},{"location":"tooling/#explorer-setup","title":"Explorer Setup","text":"<p>The <code>HollowExplorerUI</code> class in the hollow-explorer-ui project is instantiated using either a <code>HollowReadStateEngine</code> or a <code>HollowConsumer</code> and a base URL path: <pre><code>HollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowExplorerUI ui = new HollowExplorerUI(\"\", consumer);\n</code></pre></p> <p>Incoming requests should be sent to the <code>handle</code> method in your <code>HollowExplorerUI</code> instance: <pre><code>public boolean handle(String target,\n                      HttpServletRequest req,\n                      HttpServletResponse resp) throws IOException\n</code></pre></p> <p>The <code>HollowExplorerUI</code> can be used in the context of an existing web container as shown above, or can be invoked via the included <code>HollowExplorerUIServer</code>, which uses the Jetty HTTP Servlet Server: <pre><code>HollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowExplorerUIServer server = new HollowExplorerUIServer(consumer, 8080);\n\nserver.start();\nserver.join();\n</code></pre></p> <p>The above call to <code>server.join()</code> will block forever.  While the above code is running, you can point a browser to http://localhost:8080 to explore your data.</p>"},{"location":"tooling/#explorer-usage","title":"Explorer Usage","text":"<p>Upon opening your browser, you should see something like this:</p> <p></p> <p>Click on a column header to sort by that column.  This view shows details about how many records exist for each type, and the approximate heap footprint of each type.</p> <p>Click on a type to browse records.  We'll arrive at a screen like the following:</p> <p></p> <p>Clicking on the record keys on the left will display the corresponding record contents in the display field.  </p> <p>Now imagine we wanted to search for movies in which Carrie-Anne Moss starred.  On this page, click the Browse Schema link in the header to arrive at the following page:</p> <p></p> <p>The view on this page is a collapsible tree-view of the current type's schema.  Each searchable field in this view will contain a search link, which will prepopulate the search page with the type and field name.  We can navigate to the <code>Actor.actorName</code> field and click search.  We will arrive on the search page with the type and field name prepopulated:</p> <p></p> <p>We can enter the field value we are looking for and click Submit, and we will see the number of matching records of each type.  Search query matches are not limited to the directly matching records -- they are automatically rolled up to include any referencing records as well:</p> <p></p> <p>If we click on the type Movie on this page, we'll be presented with the browse view again, but this time filtered to matching records:</p> <p></p> <p>Search queries remain active in the browser session until cleared, and can be augmented to find the intersection of matches over multiple fields by entering multiple query parameters without clearing the existing ones in the session.  For example, if we want to find movies in which both Carrie-Anne Moss and the Actor with ID <code>1001</code> starred, we can go back to the search page, and enter the appropriate criteria to augment our session's query.  The results will contain only records which match both of these criteria:</p> <p></p>"},{"location":"tooling/#history-tool","title":"History tool","text":"<p>Hollow ships with a UI which can be used to browse and search changes in a dataset over time.  The history tool provides the ability to get a bird\u2019s eye view of all of the changes a dataset goes through over time, while simultaneously allowing for specific queries to see exactly how individual records change as the dataset transitions between states.  The history tool has proven to be enormously beneficial when investigating data issues in production scenarios.  When something looks incorrect, it\u2019s easy to pinpoint exactly what changed when, which can vastly expedite data corrections and eliminate hours of potential detective work.</p>"},{"location":"tooling/#history-setup","title":"History Setup","text":"<p>The <code>HollowHistoryUI</code> class in the hollow-diff-ui project can be instantiated using a <code>HollowConsumer</code> and a base URL path: <pre><code>HollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowHistoryUI ui = new HollowHistoryUI(\"\", consumer);\n</code></pre></p> <p>The <code>HollowHistoryUI</code> will by default be configured to track all of the types for which primary keys have been specified.  By default, it will track changes through the latest rolling 1024 states.  This default can be changed with another parameter in the constructor.</p> <p>Incoming requests should be sent to the <code>handle</code> method in your <code>HollowExplorerUI</code> instance: <pre><code>public boolean handle(String target,\n                      HttpServletRequest req,\n                      HttpServletResponse resp) throws IOException\n</code></pre></p> <p>The <code>HollowHistoryUI</code> can be used in the context of an existing web container as shown above, or can be invoked via the included <code>HollowHistoryUIServer</code>, which uses the Jetty HTTP Servlet Server: <pre><code>HollowConsumer consumer = ...\n\nHollowHistoryUIServer server = new HollowHistoryUIServer(consumer, 8080);\n\nserver.start();\nserver.join();\n</code></pre></p> <p>The above call to <code>server.join()</code> will block forever.  While the above code is running, you can point a browser to http://localhost:8080 to explore the history.</p>"},{"location":"tooling/#history-usage","title":"History Usage","text":"<p>Upon opening your browser, you will see something like this:</p> <p></p> <p>The history UI will track changes in types for which you have defined primary keys.  In this view, we're looking at the number of records which changed between data states.  The changes in each state are broken down here into modifications, additions, and removals.  Clicking on a state will show us a further breakdown of these changes by each top-level type:</p> <p></p> <p>Clicking on a type will show us the individual records which changed:</p> <p></p> <p>If we click on one of these records, we'll be able to inspect precisely what happened:</p> <p></p> <p>This is a collapsible tree-view in which the entire before/after state of the record is available, but only the differences are expanded by default.  Click on individual field names to expand/collapse them.</p> <p>Partially Expanded</p> <p>The double-arrows in the example above mean the field is partially expanded to highlight the diffs.  Clicking on it will fully expand the field.</p> <p>Removals show up as red, additions as green, and modifications as yellow.  The following example shows a modified actor name:</p> <p></p> <p>If this specific record key has changed in multiple states tracked by this history, those states will be highlighted on the left.  We can click back and forth through the changes to see how this record evolved over time.</p> <p>We can also search for changes in specific records by their keys.  On each page in the history tool, a textbox is available in the header labeled Lookup.   Plugging in a single field of a key of any type into this field will find matching diffs through the history:</p> <p></p> <p>Search for exact matches using composite key is also supported. When searching for a composite key, different fields need to be separated by colons.</p> <p>Clicking on individual records will bring us back to the object diff view page to see what was changed.</p>"},{"location":"tooling/#diff-tool","title":"Diff Tool","text":"<p>Just as the Hollow history tool UI makes the differences between any two adjacent states in a delta chain readily accessible, the Hollow diff tool is used to investigate the differences between any two arbitrary data states, even those which may exist in different delta chains.</p> <p>This is especially useful as a step in a regular release cadence, as the differences between data states produced, for example, in a test environment and production environment can be evaluated at a glance.  Sometimes, unintended consequences of code updates may be discovered this way, which prevents production issues before they happen.</p> <p>Initiating a diff between two data states is accomplished by loading both states into separate <code>HollowReadStateEngines</code> in memory, and then instantiating a <code>HollowDiff</code> and configuring it with the primary keys of types to diff.  For our <code>Movie</code>/<code>Actor</code> example: <pre><code>HollowConsumer testConsumer = /// load the test data\nHollowConsumer prodConsumer = /// load the prod data\n\nHollowReadStateEngine testData = testConsumer.getStateEngine();\nHollowReadStateEngine prodData = prodConsumer.getStateEngine();\n\nHollowDiff diff = new HollowDiff(testData, prodData);\ndiff.addTypeDiff(\"Movie\", \"id\");\ndiff.addTypeDiff(\"Actor\", \"actorId\");\n\ndiff.calculateDiffs();\n</code></pre></p> <p>A diff is calculated by matching records of the same type based on defined primary keys.  The unmatched records in both states are tracked, and detailed differences between field values in matching pairs are also tracked.</p> <p>Primary Keys</p> <p>The <code>HollowDiff</code> will, by default, automatically configure any primary keys which are defined in the <code>Object</code> schemas of your dataset.</p> <p>Hollow includes a ready-made UI which can be applied to a <code>HollowDiff</code>.    The <code>HollowDiffUI</code> class can be used in the context of an existing web container, or can be invoked via the <code>HollowDiffUIServer</code>, which uses the Jetty HTTP Servlet Server: <pre><code>HollowDiff diff = /// build the diff\n\nHollowDiffUIServer server = new HollowDiffUIServer(8080);\nserver.start();\n\nserver.addDiff(\"diff\", diff);\n\nserver.join();\n</code></pre></p> <p>While the above code is running, you can point a browser to http://localhost:8080 to explore the diff.</p>"},{"location":"tooling/#heap-usage-analysis","title":"Heap Usage Analysis","text":"<p>One of the most important considerations when dealing with in-memory datasets is the heap utilization of that dataset on consumer machines.  Hollow provides a number of methods to analyze this metric.</p> <p>Given a loaded <code>HollowReadStateEngine</code>, it is possible to iterate over each type and gather statistics about its approximate heap usage.  This is done in the following example: <pre><code>HollowReadStateEngine stateEngine = /// a populated state engine\n\nlong totalApproximateHeapFootprint = 0;\n\nfor(HollowTypeReadState typeState : stateEngine.getTypeStates()) {\n    String typeName = typeState.getSchema().getName();\n    long heapCost = typeState.getApproximateHeapFootprintInBytes();\n    System.out.println(typeName + \": \" + heapCost);\n    totalApproximateHeapFootprint += heapCost;\n}\n\nSystem.out.println(\"TOTAL: \" + totalApproximateHeapFootprint);\n</code></pre> As shown above, information can be gathered about the total heap footprint, and also about the heap footprint of individual types.  This information can be helpful in identifying optimization targets.  This technique can also be used to identify how the heap cost of individual types changes over time, which can provide early warning signs about optimizations which should be targeted proactively.</p>"},{"location":"tooling/#usage-tracking","title":"Usage Tracking","text":"<p>Hollow tracks usage, which can be investigated at runtime.  By default, this functionality is turned off, but it can be enabled by injecting a HollowSamplingDirector into a Hollow API in a running instance.  You can use the TimeSliceSamplingDirector implementation, which will by default record every access which happens during 1ms out of every second:</p> <pre><code>MovieAPI api = /// a custom-generated API\n\nTimeSliceSamplingDirector samplingDirector = new TimeSliceSamplingDirector();\nsamplingDirector.startSampling();\n\napi.setSamplingDirector(samplingDirector);\n</code></pre> <p>Once this is enabled, and some time has passed for samples to be gathered, the results can be collected for analysis: <pre><code>for(SampleResult result : api.getAccessSampleResults()) {\n    if(result.getNumSamples() &gt; 0)\n        System.out.println(result.getIdentifier() + \": \" +\n                                                  result.getNumSamples());\n}\n</code></pre></p>"},{"location":"tooling/#transitive-set-traverser","title":"Transitive Set Traverser","text":"<p>The <code>TransitiveSetTraverser</code> can be used to find children and parent references for a selected set of records.  We start with an initial set of selected records by ordinal, represented with a <code>Map&lt;String, BitSet&gt;</code>.  Entries in this map will indicate a type, plus the ordinals of the selected records: <pre><code>Map&lt;String, BitSet&gt; selection = new HashMap&lt;String, BitSet&gt;();\n\n/// select the movies with IDs 1 and 6.\nBitSet selectedMovies = new BitSet();\nselectedMovies.set(movieIdx.getMatchingOrdinal(1));\nselectedMovies.set(movieIdx.getMatchingOrdinal(6));\n\nselection.put(\"Movie\", movies);\n</code></pre></p> <p>We can add the references, and the transitive references of our selection.  After the following call returns, our selection will be augmented with these matches: <pre><code>TransitiveSetTraverser.addTransitiveMatches(readEngine, selection);\n</code></pre></p> <p>Transitive References</p> <p>If A references B, and B references C, then A transitively references C</p> <p>Given a selection, we can also add any records which reference anything in the selection.  This is essentially the opposite operation as above; it can be said that <code>addTransitiveMatches</code> traverses down, while <code>addReferencingOutsideClosure</code> traverses up.  After the following call returns, our selection will be augmented with this selection: <pre><code>TransitiveSetTraverser.removedReferencedOutsideClosure(readEngine, selection);\n</code></pre></p>"},{"location":"tooling/#dataset-manipulation-tools","title":"Dataset Manipulation Tools","text":""},{"location":"tooling/#filtering","title":"Filtering","text":"<p>Sometimes, a dataset will be of interest to multiple different types of consumers, but not all consumers may be interested in all aspects of a dataset.  In these cases, it\u2019s possible to omit certain types and fields from a client\u2019s view of the data.  This is typically done to tailor a consumer\u2019s heap footprint and startup time costs based on their data needs.</p> <p>Using our <code>Movie</code>/<code>Actor</code> example above, if there was a consumer which was interested in <code>Movie</code> records, but not <code>Actor</code> records, that consumer might construct a consumer-side data filter configuration in the following way: <pre><code>HollowFilterConfig config = new HollowFilterConfig(true);\nconfig.addField(\"Movie\", \"actors\");\nconfig.addType(\"ListOfActor\");\nconfig.addType(\"Actor\");\n</code></pre></p> <p>The boolean <code>true</code> parameter in the constructor above indicates that this is an exclusion filter.  We could accomplish the same goal using an inclusion filter: <pre><code>HollowFilterConfig config = new HollowFilterConfig(false);\nconfig.addField(\"Movie\", \"id\");\nconfig.addField(\"Movie\", \"title\");\nconfig.addField(\"Movie\", \"releaseYear\");\nconfig.addType(\"String\");\n</code></pre></p> <p>The difference between these two configurations is how the filter behaves as new types and fields are added to the data model.  The exclusion filter will not exclude them by default, whereas the inclusion filter will.</p> <p>A filter configuration is applied to a <code>HollowConsumer</code> at read time: <pre><code>HollowConsumer consumer = HollowConsumer.withBlobReader(reader)\n                                        .withFilterConfig(config)\n                                        .build();\n</code></pre></p>"},{"location":"tooling/#combining","title":"Combining","text":"<p>The <code>HollowCombiner</code> is used to copy data from one or more copies of hollow datasets in <code>HollowReadStateEngine</code>s into a single <code>HollowWriteStateEngine</code>.  If each of the inputs contain the same data model, the following is sufficient to combine them: <pre><code>HollowReadStateEngine input1 = /// an input\nHollowReadStateEngine input2 = /// another input\n\nHollowCombiner combiner = new HollowCombiner(input1, input2);\ncombiner.combine();\n\nHollowWriteStateEngine combined = combiner.getCombinedStateEngine();\n</code></pre></p> <p>By default, the combiner will copy all records from all types from the inputs to the output.  We can direct the combiner to exclude certain records from copying using a <code>HollowCombinerCopyDirector</code>.  The interface for a <code>HollowCombinerCopyDirector</code> allows for making decisions about copying individual records during a combine operation by implementing the following method: <pre><code>public boolean shouldCopy(HollowTypeReadState typeState, int ordinal);\n</code></pre></p> <p>If this method returns false, then the copier will not attempt to directly copy the matching record.  However, if the matching record is referenced via another record for which this method returns true, then it will still be copied regardless of the return value of this method.</p> <p>The most broadly useful provided implementation of the <code>HollowCombinerCopyDirector</code> is the <code>HollowCombinerExcludePrimaryKeysCopyDirector</code>, which can be used to specify record exclusions by primary key.  For example, if we wanted to create a copy of a state engine with the <code>Movie</code> records with ids 100 and 125 excluded: <pre><code>HollowReadStateEngine input = /// an input\nHollowPrimaryKeyIndex idx = new HollowPrimaryKeyIndex(input, \"Movie\", \"id\");\n\nHollowCombinerExcludePrimaryKeysCopyDirector director =\n                          new HollowCombinerExcludePrimaryKeysCopyDirector();\n\ndirector.excludeKey(idx, 100);\ndirector.excludeKey(idx, 125);\n\nHollowCombiner combiner = new HollowCombiner(director, input);\ncombiner.combine();\n\nHollowWriteStateEngine result = combiner.getCombineStateEngine();\n</code></pre></p> <p>It\u2019s possible that while combining two inputs, both may have a record of the same type with the same primary key.  This violation of the uniqueness constraint of a primary key can be avoided by informing the combiner of the primary keys in a data model prior to the combine operation: <pre><code>HollowCombiner combiner = new HollowCombiner(input1, input2);\n\ncombiner.setPrimaryKeys(\n        new PrimaryKey(\"Movie\", \"id\"),\n        new PrimaryKey(\"Actor\", \"actorId\")\n);\n\ncombiner.combine();\n</code></pre></p> <p>If multiple records exist in the inputs matching a single value for any of the supplied primary keys, then only one such record will be copied to the output.  The specific record which is copied will be the record from the input was supplied earliest in the constructor of the <code>HollowCombiner</code>.  Further, if any record references another record which was omitted because it would have been duplicate based on this rule, then that reference is remapped in the output state to the matching record which was chosen to be included.</p>"},{"location":"tooling/#splitting","title":"Splitting","text":"<p>A single dataset can be sharded into multiple datasets using a <code>HollowSplitter</code>.  The <code>HollowSplitter</code> takes a <code>HollowSplitterCopyDirector</code>, which indicates:</p> <ul> <li>top level types to split,</li> <li>the number of shards to create, and</li> <li>which shard to send individual records.</li> </ul> <p>Top Level Types</p> <p>Top level types are those which are not referenced by any other types.  In our <code>Movie</code>/<code>Actor</code> example, <code>Movie</code> is a top-level type, but <code>Actor</code> is not.</p> <p>Two default implementations of <code>HollowSplitterCopyDirector</code> are available:</p> <ul> <li><code>HollowSplitterOrdinalCopyDirector</code></li> <li><code>HollowSplitterPrimaryKeyCopyDirector</code>.  </li> </ul> <p>These directors will split top-level types among a specified number of shards either by ordinals or primary keys, respectively.  When splitting by ordinal, a record with a specific primary key may jump between shards when it is modified, while with the primary key director a specific primary key will consistently hash to the same shard.</p> <p>Our <code>Movie</code>/<code>Actor</code> example may use the splitter to split a dataset into four shards with the following invocation: <pre><code>HollowReadStateEngine stateEngine = /// a state engine\n\nHollowSplitterCopyDirector director =\n                            new HollowSplitterOrdinalCopyDirector(4, \"Movie\");\n\nHollowSplitter splitter = new HollowSplitter(director, stateEngine);\nsplitter.split();\n\n\nfor(int i=0; i&lt;4; i++) {\n    HollowWriteStateEngine shard = splitter.getOutputShardStateEngine(i);\n}\n</code></pre></p>"},{"location":"tooling/#state-manipulation-tools","title":"State Manipulation Tools","text":""},{"location":"tooling/#patching","title":"Patching","text":"<p>Using the <code>HollowWriteStateEngine</code>\u2019s restore capability, it\u2019s possible to produce deltas forever, so that consumers never have to load a snapshot after initialization.  However, if environmental hiccups cause a producer to fail to publish a delta, or if a delta is lost, or if it\u2019s desired to publish a delta between non-adjacent states, then the <code>HollowStateDeltaPatcher</code> may be used to produce deltas between two arbitrary states within the same delta chain.</p> <p>The <code>HollowStateDeltaPatcher</code> must produce two delta transitions to create a transition between arbitrary states.  This is because non-adjacent states may have different records occupying the same ordinals.  Since no ordinal may be removed and added in adjacent states, the state patcher must create an intermediate state in which modified records do not share any ordinals.</p> <p>See the <code>HollowStateDeltaPatcher</code> javadocs for usage details.</p>"},{"location":"validation/","title":"Validation","text":"<p>Hollow supports user-defined validation of data state. Validation can ensure that bad data does not propagate to consumers.</p> <p>Validation occurs after data state is published (as blobs) and before the availability of  the data state is announced, thereby allowing inspection of erroneous but published data state.</p>"},{"location":"validation/#implementing-and-registering-a-validator","title":"Implementing and registering a validator","text":"<p>A validator is created by implementing the interface <code>com.netflix.hollow.api.producer.validation.ValidatorListener</code>. </p> <p>The following is an example validator that checks if there are two or more objects that  have the same primary key:</p> <p><pre><code>public static class DuplicateValidator implements ValidatorListener {\n\n    private final String dataTypeName;\n\n    public DuplicateDataDetectionValidator(String dataTypeName) {\n        this.dataTypeName = Objects.requireNonNull(dataTypeName);\n    }\n\n    @Override\n    public String getName() {\n        return this.getClass().getName() + \":\" + dataTypeName;\n    }\n\n    @Override\n    public ValidationResult onValidate(HollowProducer.ReadState dataState) {\n        ValidationResult.ValidationResultBuilder vrb = ValidationResult.from(this);\n        vrb.detail(\"Type\", dataTypeName);\n\n        HollowSchema schema = dataState.getStateEngine().getSchema(dataTypeName);\n        if (schema.getSchemaType() != HollowSchema.SchemaType.OBJECT) {\n            return vrb.failed(\"Bad configuration: data type is not an Object\");\n        }\n\n        HollowObjectSchema oSchema = (HollowObjectSchema) schema;\n        PrimaryKey primaryKey = oSchema.getPrimaryKey();\n        if (primaryKey == null) {\n            return vrb.failed(\"Bad configuration: data type does not have a primary key\");\n        }\n        vrb.detail(\"PrimaryKey\", primaryKey);\n\n        Collection&lt;Object[]&gt; duplicateKeys = getDuplicateKeys(dataState.getStateEngine(), primaryKey);\n        if (!duplicateKeys.isEmpty()) {\n            return vrb\n                    .detail(\"Objects\",\n                            duplicateKeys.stream().map(Arrays::toString).collect(Collectors.joining(\",\")))\n                    .failed(\"Duplicate objects with the same primary key\");\n        }\n\n        return vrb.passed();\n    }\n\n    private Collection&lt;Object[]&gt; getDuplicateKeys(HollowReadStateEngine stateEngine, PrimaryKey primaryKey) {\n        ...\n    }\n}\n</code></pre> The following highlights some important aspects (without going into the specific details of how  this validator detects duplicate objects with the same primary key):</p> <ul> <li> <p>A validator has state, in this case the name of data type of object instances it is checking. The name of the validator, returned from the call to <code>getName</code>, should include pertinent state to  differentiate between two or more registered instances of the same validator class.</p> </li> <li> <p>When validation is to be performed the producer will emit a validate event by calling  the <code>onValidate</code> method, with the data state, for all registered validators.</p> </li> <li> <p>A validator builds and returns a <code>ValidationResult</code> reporting the name of the validator and whether validation passed or failed. Details may be included to provide additional information, such as if the duplicate objects to help resolve the problem of the bad data.</p> </li> <li> <p>If the validator throws an unexpected runtime exception then it is as if a  <code>ValidationResult</code> is returned reporting error with that exception.</p> </li> </ul> <p>The example validator presented above may be registered when building a <code>HollowProducer</code>:</p> <pre><code>HollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .withValidator(new DuplicateValidator(\"Movie\"))\n                                        .build();\n\nproducer.runCycle(state -&gt; {\n    for(Movie movie : movies)\n        state.add(movie);\n});\n</code></pre>"},{"location":"validation/#pre-defined-validators","title":"Pre-defined validators","text":"<p>It is not necessarily easy to write a validator operating on the data state using an  instance of <code>ReadState</code>.  To make it easier for developers Hollow provides a few pre-defined validators for common use cases:</p> <ol> <li>A duplicate object validator (similar to the example above), <code>DuplicateDataDetectionValidator</code>.</li> <li>A record cound variance validator, <code>RecordCountVarianceValidator</code>. This validator can be configured to check if cardinality of objects varies within a required  percentage.  This is useful to detect if the number of objects of a particular data type  unexpectedly decreases or increases.</li> <li>An object modification validator, <code>ObjectModificationValidator</code>. This validator can be configured to compare the state of objects (with the same primary key) that have been modified (not added or removed).</li> <li>A minimum record count validator, <code>MinimumRecordCountValidator</code>. This validator can be configured to guard against large deletions, by setting a threshold for  allowable minimum number of records in a data type.</li> <li>A null primary key field validator, <code>NullPrimaryKeyFieldValidator</code>. This validator can be configured to check if any fields of an object that are part of the primary key are null. This is useful to enforce the non-nullability of primary key fields.</li> </ol>"},{"location":"validation/#using-the-generated-object-api","title":"Using the generated object API","text":"<p>Ordinarily the generated object API is utilized by consumers, but there is no inherent reason why it cannot also be utilized by the producer.  This can make it significantly easier to write a custom validator.  For example, the <code>MovieAPI</code> may be created from data state and all movies traversed as follows:</p> <pre><code>public static class MyValidator implements ValidatorListener {\n\n    @Override\n    public String getName() { ... }\n\n    @Override\n    public ValidationResult onValidate(HollowProducer.ReadState dataState) {\n        MovieAPI api = new MovieAPI(dataState);\n\n        for (Movie m : api.getAllMovie()) {\n            ...\n        }\n    }\n}\n</code></pre> <p>The <code>ObjectModificationValidator</code> is designed to be used with a generated object API.</p>"},{"location":"validation/#listening-to-events-emitted-by-the-producer-for-other-stages","title":"Listening to events emitted by the producer for other stages","text":"<p>A registered validator, implementing <code>ValidatorListener</code>, may also receive events for other  stages by implementing other listener interfaces.  For example implementing the <code>CycleListener</code>  will enable the validator to receive events for when a production cycle is started and completed. Receiving an event when the cycle starts (a call to the <code>onCycleStart</code>) method may enable the validator to load validator configuration state dynamically and freeze that state for the  duration of the cycle.</p>"},{"location":"validation/#integrity-checking","title":"Integrity checking","text":"<p>Hollow additionally supports a special form of validation, integrity checking of data state,  that occurs before the user-defined validation of data state.  </p> <p>The integrity checking ensures that the data state is not corrupted, perhaps due to a bug. Failure is likely rare but an important safety check to ensure corrupt data is not propagated to consumers.</p>"}]}